# DeepModeling-Infer é¡¹ç›®æ€»ç»“

## ğŸ¯ é¡¹ç›®ç›®æ ‡

åˆ›å»ºä¸€ä¸ª**çº¯æ¨ç†æ¡†æ¶**ï¼Œç”¨äºæµ‹è¯•è®­ç»ƒå¥½çš„ SFT å’Œ RL æ¨¡å‹ï¼Œå®Œå…¨å…¼å®¹æ‚¨ç°æœ‰çš„å‘½ä»¤è¡Œæ¥å£ï¼š

```bash
python main.py --workflow scientific --benchmark mathmodeling \
    --data-dir "/path/to/data" \
    --llm-api http://localhost:8000 \
    --task mathmodeling-0
```

## âœ… å·²å®Œæˆçš„åŠŸèƒ½

### 1. æ ¸å¿ƒæ¨ç†å¼•æ“
- âœ… **DeepModelingInferenceAgent** - æ”¯æŒ API å’Œæœ¬åœ°æ¨¡å‹
- âœ… **Scientific Method å·¥ä½œæµ** - å®Œæ•´çš„ç§‘å­¦æ–¹æ³•å¾ªç¯
- âœ… **æ²™ç®±æ‰§è¡Œ** - å®‰å…¨çš„ä»£ç æ‰§è¡Œç¯å¢ƒ
- âœ… **è‡ªåŠ¨è¯„åˆ†** - ä½¿ç”¨åŸºå‡†åŸç”Ÿè¯„åˆ†ç³»ç»Ÿ

### 2. å¤šç§ä½¿ç”¨æ–¹å¼
- âœ… **main.py** - å…¼å®¹æ‚¨çš„å‘½ä»¤è¡Œæ¥å£
- âœ… **src.infer** - çµæ´»çš„æ¨ç†æ¨¡å—
- âœ… **deploy_and_test.sh** - ä¸€é”®éƒ¨ç½²å’Œæµ‹è¯•
- âœ… **quickstart.sh** - å¿«é€Ÿå¯åŠ¨è„šæœ¬
- âœ… **Python API** - ä»£ç é›†æˆæ¥å£

### 3. å®Œæ•´æ–‡æ¡£
- âœ… **README.md** - å®Œæ•´åŠŸèƒ½æ–‡æ¡£ (8.7KB)
- âœ… **QUICKSTART.md** - å¿«é€Ÿå¼€å§‹æŒ‡å— (5.2KB)
- âœ… **DEPLOYMENT_GUIDE.md** - è¯¦ç»†éƒ¨ç½²æŒ‡å—
- âœ… **PROJECT_OVERVIEW.md** - é¡¹ç›®è®¾è®¡æ–‡æ¡£ (7.5KB)
- âœ… **USAGE_SUMMARY.md** - ä½¿ç”¨æ–¹å¼æ€»ç»“
- âœ… **example_usage.py** - Python ä»£ç ç¤ºä¾‹

### 4. æ”¯æŒçš„åŸºå‡†
- âœ… Engineering Benchmark
- âœ… MathModeling Benchmark
- âœ… Science Benchmark
- âœ… MLE Benchmark

### 5. éƒ¨ç½²æ–¹å¼
- âœ… vLLM (æ¨è)
- âœ… LLM Lite
- âœ… TGI (Text Generation Inference)
- âœ… æœ¬åœ° transformers

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
deepmodeling-infer/
â”œâ”€â”€ main.py â­                    # å…¼å®¹çš„ä¸»å…¥å£ï¼ˆæ‚¨ç†Ÿæ‚‰çš„æ¥å£ï¼‰
â”œâ”€â”€ deploy_and_test.sh â­         # ä¸€é”®éƒ¨ç½²å’Œæµ‹è¯•è„šæœ¬
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py              # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ config.py                # æ¨ç†é…ç½®
â”‚   â”œâ”€â”€ infer.py                 # æ¨ç†å¼•æ“ (22KB)
â”‚   â”œâ”€â”€ utils.py                 # å·¥å…·å‡½æ•°ï¼ˆè¯„åˆ†ï¼‰
â”‚   â””â”€â”€ data_utils.py            # æ•°æ®åŠ è½½
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ quickstart.sh            # å¿«é€Ÿå¯åŠ¨
â”‚   â””â”€â”€ run_infer.sh             # å®Œæ•´è¿è¡Œè„šæœ¬
â”‚
â”œâ”€â”€ docs/ (æ–‡æ¡£)
â”‚   â”œâ”€â”€ README.md â­              # ä¸»æ–‡æ¡£
â”‚   â”œâ”€â”€ QUICKSTART.md            # å¿«é€Ÿå¼€å§‹
â”‚   â”œâ”€â”€ DEPLOYMENT_GUIDE.md â­    # éƒ¨ç½²æŒ‡å—
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md      # é¡¹ç›®æ¦‚è§ˆ
â”‚   â”œâ”€â”€ USAGE_SUMMARY.md â­       # ä½¿ç”¨æ€»ç»“
â”‚   â””â”€â”€ PROJECT_SUMMARY.md       # æœ¬æ–‡ä»¶
â”‚
â”œâ”€â”€ example_usage.py             # Python ç¤ºä¾‹
â”œâ”€â”€ requirements.txt             # ä¾èµ–åˆ—è¡¨
â””â”€â”€ .gitignore                   # Git é…ç½®
```

â­ æ ‡è®°çš„æ˜¯æœ€é‡è¦çš„æ–‡ä»¶

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ3 æ­¥ï¼‰

### æ–¹å¼ A: è‡ªåŠ¨åŒ–è„šæœ¬ï¼ˆæœ€ç®€å•ï¼‰

```bash
# ä¸€è¡Œå‘½ä»¤å®Œæˆéƒ¨ç½²å’Œæµ‹è¯•
./deploy_and_test.sh --model /path/to/model --task industry-0
```

### æ–¹å¼ B: æ‰‹åŠ¨éƒ¨ç½²ï¼ˆæ›´çµæ´»ï¼‰

```bash
# 1. éƒ¨ç½²æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/model --port 8000 &

# 2. è¿è¡Œæµ‹è¯•
python main.py --workflow scientific --benchmark engineering \
    --llm-api http://localhost:8000 --task industry-0

# 3. æŸ¥çœ‹ç»“æœ
cat results/summary_*.json | jq .
```

---

## ğŸ’¡ æ ¸å¿ƒç‰¹æ€§

### 1. å®Œå…¨å…¼å®¹æ‚¨çš„æ¥å£

**æ‚¨çš„åŸå§‹å‘½ä»¤**:
```bash
python main.py --workflow scientific --benchmark mathmodeling \
    --data-dir "/path/to/data" \
    --llm-model openai/deepseek-ai/DeepSeek-V3-Terminus \
    --task mathmodeling-0
```

**ç°åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨** âœ…:
```bash
python main.py --workflow scientific --benchmark mathmodeling \
    --data-dir "/path/to/data" \
    --llm-api http://localhost:8000 \
    --task mathmodeling-0
```

### 2. æ— è®­ç»ƒä¾èµ–

- âŒ ä¸éœ€è¦ Agent-Lightning
- âŒ ä¸éœ€è¦ VERL
- âŒ ä¸éœ€è¦ Ray
- âŒ ä¸éœ€è¦ reward_function
- âœ… åªéœ€æœ€å°åŒ–ä¾èµ–ï¼ˆrequests, pandas, transformers å¯é€‰ï¼‰

### 3. çµæ´»çš„æ¨¡å‹æ”¯æŒ

```bash
# API æ¨¡å¼ï¼ˆæ¨èï¼‰
--llm-api http://localhost:8000

# æœ¬åœ°æ¨¡å‹
--model-path /path/to/model --use-local-model

# OpenAI æ ¼å¼
--llm-model openai/provider/model-name
```

### 4. è‡ªåŠ¨è¯„åˆ†

ä½¿ç”¨åŸºå‡†åŸç”Ÿè¯„åˆ†ç³»ç»Ÿï¼Œæ— éœ€é¢å¤–é…ç½®ï¼š

```python
# è‡ªåŠ¨è°ƒç”¨å¯¹åº”åŸºå‡†çš„è¯„åˆ†å‡½æ•°
grader = get_grader()
score = grader.grade_submission(task, submission_path)
```

---

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: å•ä»»åŠ¡æµ‹è¯•

```bash
python main.py --workflow scientific --benchmark engineering \
    --llm-api http://localhost:8000 --task industry-0
```

### ç¤ºä¾‹ 2: æ‰¹é‡æµ‹è¯•

```bash
python main.py --workflow scientific --benchmark engineering \
    --llm-api http://localhost:8000 --task-limit 10
```

### ç¤ºä¾‹ 3: MathModeling åŸºå‡†

```bash
python main.py --workflow scientific --benchmark mathmodeling \
    --data-dir "/home/aiops/liufan/projects/DeepModeling/data/mathmodeling-bench/competitions" \
    --llm-api http://localhost:8000 --task mathmodeling-0
```

### ç¤ºä¾‹ 4: å¯¹æ¯” SFT vs RL

```bash
# æµ‹è¯• SFT
./deploy_and_test.sh --model /path/to/sft --task-limit 10
mv results results_sft

# æµ‹è¯• RL
./deploy_and_test.sh --model /path/to/rl --task-limit 10
mv results results_rl

# æ¯”è¾ƒ
echo "SFT: $(cat results_sft/summary_*.json | jq .avg_grade)"
echo "RL:  $(cat results_rl/summary_*.json | jq .avg_grade)"
```

---

## ğŸ“ˆ è¾“å‡ºç»“æœ

### 1. å®æ—¶è¾“å‡º

```
================================================================================
Task 1/5: industry-0
================================================================================
[INFO] Starting: run_name=infer_run_industry_0_abc12345
[TURN 1/10] Calling LLM...
[TURN 1/10] Executing experiment (1234 chars)
[TURN 1/10] Execution: âœ“
[TURN 2/10] Calling LLM...
...
[INFER] Completed 6 turns, success=True, duration=245.32s
[INFER] Grade score: 0.8523

âœ“ Task completed:
  Success: True
  Turns: 6
  Grade: 0.8523
  Duration: 245.32s
```

### 2. æ±‡æ€»ç»“æœ (results/summary_*.json)

```json
{
  "workflow": "scientific",
  "benchmark": "engineering",
  "total_tasks": 5,
  "successful": 4,
  "failed": 1,
  "success_rate": 80.0,
  "avg_turns": 6.2,
  "avg_grade": 0.7823,
  "results": [...]
}
```

### 3. è¯¦ç»†æ—¥å¿— (workspace_infer/)

```
workspace_infer/infer_run_industry_0_abc12345/
â”œâ”€â”€ sandbox_workdir/
â”‚   â”œâ”€â”€ data/              # é“¾æ¥çš„æ•°æ®
â”‚   â””â”€â”€ submission.csv     # ç”Ÿæˆçš„æäº¤æ–‡ä»¶
â””â”€â”€ artifacts/telemetry/
    â”œâ”€â”€ conversation.jsonl # å®Œæ•´å¯¹è¯
    â””â”€â”€ run_metadata.json  # è¿è¡Œå…ƒæ•°æ®
```

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰é…ç½®

```bash
python main.py \
    --workflow scientific \
    --benchmark engineering \
    --llm-api http://localhost:8000 \
    --task industry-0 \
    --max-turns 15 \
    --temperature 0.3 \
    --sandbox-timeout 1200
```

### 2. æ‰¹é‡å¹¶è¡Œæµ‹è¯•

```bash
# å¯åŠ¨å¤šä¸ªå®ä¾‹
for i in {0..4}; do
    python main.py \
        --llm-api http://localhost:8000 \
        --task industry-$i \
        --output-dir results/task_$i &
done
wait
```

### 3. Python é›†æˆ

```python
from src import DeepModelingInferenceAgent, load_benchmark_tasks

agent = DeepModelingInferenceAgent(
    api_endpoint="http://localhost:8000",
    max_turns=10,
)

tasks = load_benchmark_tasks("engineering", limit=5)
for task in tasks:
    result = agent.run_inference(task)
    print(f"{task['task_id']}: {result['grade_score']}")
```

---

## ğŸ“š æ–‡æ¡£å¯¼èˆª

| æ–‡æ¡£ | å†…å®¹ | é€‚ç”¨å¯¹è±¡ |
|------|------|---------|
| README.md | å®Œæ•´åŠŸèƒ½æ–‡æ¡£ | æ‰€æœ‰ç”¨æˆ· |
| QUICKSTART.md | 10 ä¸ªå¿«é€Ÿç¤ºä¾‹ | åˆå­¦è€… |
| DEPLOYMENT_GUIDE.md | è¯¦ç»†éƒ¨ç½²æ­¥éª¤ | éƒ¨ç½²äººå‘˜ |
| USAGE_SUMMARY.md | 5 ç§ä½¿ç”¨æ–¹å¼ | æ—¥å¸¸ç”¨æˆ· |
| PROJECT_OVERVIEW.md | è®¾è®¡å’Œæ¶æ„ | å¼€å‘è€… |
| example_usage.py | Python ä»£ç ç¤ºä¾‹ | å¼€å‘è€… |

---

## ğŸ“ å­¦ä¹ è·¯å¾„

### åˆå­¦è€…
1. é˜…è¯» **QUICKSTART.md**
2. è¿è¡Œ `./deploy_and_test.sh --model your-model --task task-id`
3. æŸ¥çœ‹ `results/summary_*.json`

### æ—¥å¸¸ç”¨æˆ·
1. é˜…è¯» **DEPLOYMENT_GUIDE.md**
2. éƒ¨ç½² vLLM: `python -m vllm.entrypoints.openai.api_server ...`
3. ä½¿ç”¨ `main.py` è¿›è¡Œæµ‹è¯•

### å¼€å‘è€…
1. é˜…è¯» **PROJECT_OVERVIEW.md**
2. æŸ¥çœ‹ **example_usage.py**
3. å¯¼å…¥ `src` æ¨¡å—è‡ªå®šä¹‰å·¥ä½œæµ

---

## âœ¨ ä¸ deepmodeling-rl çš„å¯¹æ¯”

| ç‰¹æ€§ | deepmodeling-rl | deepmodeling-infer |
|------|----------------|-------------------|
| **ç”¨é€”** | è®­ç»ƒæ¨¡å‹ | æµ‹è¯•æ¨¡å‹ |
| **ä¾èµ–** | Agent-Lightning + VERL + Ray | requests + pandas |
| **å‘½ä»¤è¡Œ** | è®­ç»ƒç‰¹å®šæ¥å£ | å…¼å®¹æ‚¨çš„æ¥å£ âœ“ |
| **æ¨¡å‹åŠ è½½** | VERL åˆ†å¸ƒå¼ | API æˆ–æœ¬åœ° |
| **å¥–åŠ±å‡½æ•°** | å¿…éœ€ | ä¸éœ€è¦ âœ“ |
| **éƒ¨ç½²** | å¤æ‚ | ç®€å•ï¼ˆvLLMï¼‰ âœ“ |
| **è¾“å‡º** | Checkpoints | è¯„æµ‹ç»“æœ |

---

## ğŸ› ï¸ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: API è¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥ API
curl http://localhost:8000/v1/models

# æŸ¥çœ‹æ—¥å¿—
tail -f vllm.log
```

### é—®é¢˜ 2: æ‰¾ä¸åˆ°æ•°æ®
```bash
# æŒ‡å®šæ•°æ®è·¯å¾„
python main.py --data-dir /path/to/data ...
```

### é—®é¢˜ 3: å†…å­˜ä¸è¶³
```bash
# ä½¿ç”¨æ›´å°çš„ GPU åˆ©ç”¨ç‡
python -m vllm.entrypoints.openai.api_server \
    --model your-model \
    --gpu-memory-utilization 0.7
```

---

## ğŸ“ è·å–å¸®åŠ©

```bash
# æŸ¥çœ‹ä¸»ç¨‹åºå¸®åŠ©
python main.py --help

# æŸ¥çœ‹æ¨ç†æ¨¡å—å¸®åŠ©
python -m src.infer --help

# æŸ¥çœ‹éƒ¨ç½²è„šæœ¬å¸®åŠ©
./deploy_and_test.sh --help
```

---

## ğŸ‰ æ€»ç»“

æ‚¨ç°åœ¨æ‹¥æœ‰ä¸€ä¸ª**å®Œæ•´çš„æ¨ç†æ¡†æ¶**ï¼š

âœ… **å…¼å®¹æ‚¨çš„æ¥å£** - ç›´æ¥ä½¿ç”¨ `main.py`  
âœ… **ç®€å•éƒ¨ç½²** - vLLM ä¸€è¡Œå‘½ä»¤  
âœ… **è‡ªåŠ¨è¯„åˆ†** - åŸºå‡†åŸç”Ÿè¯„åˆ†  
âœ… **è¯¦ç»†æ—¥å¿—** - å®Œæ•´æ¨ç†è®°å½•  
âœ… **å¤šç§æ–¹å¼** - 5 ç§ä½¿ç”¨æ–¹å¼  
âœ… **å®Œæ•´æ–‡æ¡£** - 6 ä»½è¯¦ç»†æ–‡æ¡£  

**å¼€å§‹ä½¿ç”¨**:
```bash
cd /home/aiops/liufan/projects/DeepModeling/examples/deepmodeling-infer
./deploy_and_test.sh --model /path/to/model --task industry-0
```

ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼ ğŸš€
