# ä½¿ç”¨æ–¹å¼æ€»ç»“

deepmodeling-infer æä¾›å¤šç§ä½¿ç”¨æ–¹å¼ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚ã€‚

---

## æ–¹å¼ 1: å…¼å®¹çš„ main.py æ¥å£ï¼ˆæ¨èï¼‰

**é€‚ç”¨åœºæ™¯**: ä¸ç°æœ‰å·¥ä½œæµå…¼å®¹ï¼Œç†Ÿæ‚‰çš„å‘½ä»¤è¡Œæ¥å£

### åŸºæœ¬ç”¨æ³•

```bash
python main.py --workflow scientific --benchmark <BENCHMARK> \
    --llm-api <API_ENDPOINT> --task <TASK_ID>
```

### å®Œæ•´ç¤ºä¾‹

```bash
# Engineering Benchmark
python main.py --workflow scientific --benchmark engineering \
    --llm-api http://localhost:8000 --task industry-0

# MathModeling Benchmark
python main.py --workflow scientific --benchmark mathmodeling \
    --data-dir "/path/to/data" \
    --llm-api http://localhost:8000 --task mathmodeling-0

# æ‰¹é‡æµ‹è¯•
python main.py --workflow scientific --benchmark engineering \
    --llm-api http://localhost:8000 --task-limit 5
```

### å‚æ•°è¯´æ˜

- `--workflow scientific`: å·¥ä½œæµç±»å‹ï¼ˆå›ºå®šï¼‰
- `--benchmark`: åŸºå‡†åç§°ï¼ˆengineering, mathmodeling, science, mleï¼‰
- `--llm-api`: API endpoint
- `--task`: å•ä¸ªä»»åŠ¡ ID
- `--task-limit`: æ‰¹é‡æµ‹è¯•ä»»åŠ¡æ•°é‡
- `--data-dir`: è‡ªå®šä¹‰æ•°æ®ç›®å½•ï¼ˆå¯é€‰ï¼‰

---

## æ–¹å¼ 2: ç›´æ¥ä½¿ç”¨ src.infer æ¨¡å—

**é€‚ç”¨åœºæ™¯**: éœ€è¦æ›´å¤šæ§åˆ¶é€‰é¡¹ï¼Œé«˜çº§ç”¨æ³•

### åŸºæœ¬ç”¨æ³•

```bash
python -m src.infer --api-endpoint <ENDPOINT> --task-id <TASK>
```

### å®Œæ•´ç¤ºä¾‹

```bash
# API æ¨¡å¼
python -m src.infer \
    --api-endpoint http://localhost:8000 \
    --benchmark engineering \
    --task-id industry-0 \
    --max-turns 10 \
    --temperature 0.0

# æœ¬åœ°æ¨¡å‹
python -m src.infer \
    --model-path /path/to/model \
    --use-local-model \
    --task-id industry-0

# æ‰¹é‡æµ‹è¯•ç‰¹å®šä»»åŠ¡
python -m src.infer \
    --api-endpoint http://localhost:8000 \
    --competitions industry-0 industry-1 industry-2
```

---

## æ–¹å¼ 3: ä¸€é”®éƒ¨ç½²å’Œæµ‹è¯•è„šæœ¬

**é€‚ç”¨åœºæ™¯**: å¿«é€Ÿæµ‹è¯•ï¼Œè‡ªåŠ¨åŒ–éƒ¨ç½²

### åŸºæœ¬ç”¨æ³•

```bash
./deploy_and_test.sh --model <MODEL_PATH> --task <TASK_ID>
```

### å®Œæ•´ç¤ºä¾‹

```bash
# éƒ¨ç½²æ¨¡å‹å¹¶æµ‹è¯•å•ä¸ªä»»åŠ¡
./deploy_and_test.sh \
    --model /path/to/model \
    --task industry-0

# éƒ¨ç½²å¹¶æ‰¹é‡æµ‹è¯•
./deploy_and_test.sh \
    --model /path/to/model \
    --task-limit 5 \
    --max-turns 8

# ä½¿ç”¨å·²éƒ¨ç½²çš„ API
./deploy_and_test.sh \
    --skip-deploy \
    --port 8000 \
    --task industry-0

# è‡ªå®šä¹‰é…ç½®
./deploy_and_test.sh \
    --model /path/to/model \
    --benchmark mathmodeling \
    --task mathmodeling-0 \
    --temperature 0.3 \
    --max-turns 15
```

### ç‰¹ç‚¹

- âœ… è‡ªåŠ¨éƒ¨ç½² vLLM
- âœ… è‡ªåŠ¨ç­‰å¾… API å°±ç»ª
- âœ… è‡ªåŠ¨éªŒè¯ API
- âœ… è¿è¡Œåæç¤ºæ¸…ç†

---

## æ–¹å¼ 4: å¿«é€Ÿå¯åŠ¨è„šæœ¬

**é€‚ç”¨åœºæ™¯**: ç®€å•å¿«é€Ÿæµ‹è¯•

### åŸºæœ¬ç”¨æ³•

```bash
./scripts/quickstart.sh <MODEL_PATH> <TASK_ID> [options]
```

### ç¤ºä¾‹

```bash
# API æ¨¡å¼
./scripts/quickstart.sh dummy industry-0 --api-endpoint http://localhost:8000

# æœ¬åœ°æ¨¡å‹
./scripts/quickstart.sh /path/to/model industry-0 --use-local-model

# æ‰¹é‡æµ‹è¯•
./scripts/quickstart.sh dummy --api-endpoint http://localhost:8000 --task-limit 3
```

---

## æ–¹å¼ 5: Python ä»£ç è°ƒç”¨

**é€‚ç”¨åœºæ™¯**: é›†æˆåˆ°å…¶ä»– Python é¡¹ç›®ï¼Œè‡ªå®šä¹‰å·¥ä½œæµ

### åŸºæœ¬ç”¨æ³•

```python
from src import DeepModelingInferenceAgent, load_benchmark_tasks

# åˆ›å»º agent
agent = DeepModelingInferenceAgent(
    api_endpoint="http://localhost:8000",
    max_turns=10,
    temperature=0.0,
)

# åŠ è½½ä»»åŠ¡
tasks = load_benchmark_tasks(
    benchmark="engineering",
    competitions=["industry-0"],
)

# è¿è¡Œæ¨ç†
result = agent.run_inference(tasks[0])

# æŸ¥çœ‹ç»“æœ
print(f"Success: {result['success']}")
print(f"Grade: {result['grade_score']}")
```

### å®Œæ•´ç¤ºä¾‹

å‚è€ƒ `example_usage.py` æ–‡ä»¶ä¸­çš„ç¤ºä¾‹ä»£ç ã€‚

---

## å„ç§æ–¹å¼å¯¹æ¯”

| æ–¹å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----|------|------|---------|
| main.py | å…¼å®¹ç°æœ‰æ¥å£ | å‚æ•°å›ºå®š | æ—¥å¸¸ä½¿ç”¨ |
| src.infer | çµæ´»æ§åˆ¶ | å‘½ä»¤è¾ƒé•¿ | é«˜çº§ç”¨æ³• |
| deploy_and_test.sh | å…¨è‡ªåŠ¨ | ä¾èµ– bash | å¿«é€Ÿæµ‹è¯• |
| quickstart.sh | ç®€å•å¿«é€Ÿ | åŠŸèƒ½æœ‰é™ | åˆæ¬¡å°è¯• |
| Python API | å®Œå…¨æ§åˆ¶ | éœ€è¦å†™ä»£ç  | é›†æˆå¼€å‘ |

---

## å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: å¿«é€ŸéªŒè¯æ¨¡å‹

```bash
# æœ€ç®€å•çš„æ–¹å¼
./deploy_and_test.sh --model /path/to/model --task industry-0
```

### åœºæ™¯ 2: æ—¥å¸¸å¼€å‘æµ‹è¯•

```bash
# ä½¿ç”¨ main.py
python main.py --workflow scientific --benchmark engineering \
    --llm-api http://localhost:8000 --task-limit 3
```

### åœºæ™¯ 3: æ‰¹é‡è¯„æµ‹

```bash
# ä½¿ç”¨ src.infer
python -m src.infer \
    --api-endpoint http://localhost:8000 \
    --benchmark engineering \
    --task-limit 20 \
    --output-dir results/batch_eval
```

### åœºæ™¯ 4: å¯¹æ¯”ä¸åŒæ¨¡å‹

```bash
# SFT æ¨¡å‹
./deploy_and_test.sh --model /path/to/sft --task-limit 10
mv results results_sft

# RL æ¨¡å‹
./deploy_and_test.sh --model /path/to/rl --task-limit 10
mv results results_rl

# æ¯”è¾ƒ
python scripts/compare_results.py results_sft results_rl
```

### åœºæ™¯ 5: é›†æˆåˆ° CI/CD

```bash
# è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬
#!/bin/bash
set -e

# éƒ¨ç½²æœ€æ–°æ¨¡å‹
./deploy_and_test.sh \
    --model /path/to/latest/checkpoint \
    --task-limit 5 \
    --max-turns 8

# æ£€æŸ¥ç»“æœ
SCORE=$(cat results/summary_*.json | jq '.avg_grade')
if (( $(echo "$SCORE < 0.7" | bc -l) )); then
    echo "Performance degradation detected!"
    exit 1
fi
```

---

## æ¨èå·¥ä½œæµ

### å¯¹äºåˆå­¦è€…

```bash
1. ä½¿ç”¨ quickstart.sh å¿«é€Ÿå°è¯•
   ./scripts/quickstart.sh your-model task-id --api-endpoint http://localhost:8000

2. æŸ¥çœ‹ç»“æœ
   cat results/summary_*.json | jq .

3. æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
   ls workspace_infer/
```

### å¯¹äºæ—¥å¸¸ä½¿ç”¨

```bash
1. éƒ¨ç½²æ¨¡å‹ï¼ˆä¸€æ¬¡ï¼‰
   python -m vllm.entrypoints.openai.api_server --model your-model --port 8000 &

2. ä½¿ç”¨ main.py è¿›è¡Œæµ‹è¯•ï¼ˆå¤šæ¬¡ï¼‰
   python main.py --workflow scientific --benchmark engineering \
       --llm-api http://localhost:8000 --task industry-0

3. éœ€è¦æ—¶åœæ­¢æœåŠ¡
   lsof -ti:8000 | xargs kill -9
```

### å¯¹äºæ‰¹é‡è¯„æµ‹

```bash
1. ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬
   ./deploy_and_test.sh --model your-model --task-limit 20

2. æˆ–æ‰‹åŠ¨æ§åˆ¶
   # éƒ¨ç½²
   python -m vllm.entrypoints.openai.api_server --model your-model --port 8000 &
   
   # æ‰¹é‡æµ‹è¯•
   python -m src.infer --api-endpoint http://localhost:8000 --task-limit 50
   
   # åœæ­¢
   lsof -ti:8000 | xargs kill -9
```

---

## è·å–å¸®åŠ©

```bash
# main.py å¸®åŠ©
python main.py --help

# src.infer å¸®åŠ©
python -m src.infer --help

# deploy_and_test.sh å¸®åŠ©
./deploy_and_test.sh --help

# quickstart.sh å¸®åŠ©
./scripts/quickstart.sh
```

---

## æ–‡æ¡£å¯¼èˆª

- **README.md** - å®Œæ•´åŠŸèƒ½æ–‡æ¡£
- **QUICKSTART.md** - 10 ä¸ªå¿«é€Ÿå¼€å§‹ç¤ºä¾‹
- **DEPLOYMENT_GUIDE.md** - è¯¦ç»†éƒ¨ç½²æŒ‡å—
- **PROJECT_OVERVIEW.md** - é¡¹ç›®æ¶æ„è®¾è®¡
- **USAGE_SUMMARY.md** - æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨æ–¹å¼æ€»ç»“

---

## æ€»ç»“

é€‰æ‹©åˆé€‚çš„æ–¹å¼ï¼š

- ğŸš€ **æƒ³è¦æœ€å¿«**: `./deploy_and_test.sh --model your-model --task task-id`
- ğŸ’¼ **æ—¥å¸¸ä½¿ç”¨**: `python main.py --workflow scientific ...`
- ğŸ”§ **é«˜çº§æ§åˆ¶**: `python -m src.infer ...`
- ğŸ **Python é›†æˆ**: å¯¼å…¥ `src` æ¨¡å—ä½¿ç”¨

æ‰€æœ‰æ–¹å¼éƒ½æ”¯æŒç›¸åŒçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œåªæ˜¯æ¥å£å’Œä¾¿åˆ©æ€§ä¸åŒã€‚
