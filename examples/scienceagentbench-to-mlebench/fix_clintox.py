#!/usr/bin/env python3
"""
ä¿®æ­£ sciencebench-001-clintox-nn ä»»åŠ¡

æ­£ç¡®ä½¿ç”¨ gold_results å’Œ eval_programs çš„é€»è¾‘
"""

from pathlib import Path


# ä¿®æ­£åçš„ prepare.py
PREPARE_PY_FIXED = '''"""
Data preparation for ScienceBench task 1 (Clintox)

æ­£ç¡®ç‰ˆæœ¬ï¼šä½¿ç”¨ gold_results ä½œä¸ºç­”æ¡ˆ
"""

import pandas as pd
from pathlib import Path


def prepare(raw: Path, public: Path, private: Path):
    """
    Prepare the Clintox dataset.

    æ”¹è¿›ï¼š
    1. ä» gold_results è·å–æµ‹è¯•é›†ç­”æ¡ˆ
    2. æµ‹è¯•é›†åªæä¾› smilesï¼ˆç§»é™¤æ ‡ç­¾ï¼‰
    3. ç­”æ¡ˆä¿å­˜åˆ° private/

    Args:
        raw: Path to datasets/clintox/
        public: Path to public directory
        private: Path to private directory
    """
    print("Preparing Clintox dataset (FIXED VERSION)...")

    # 1. è¯»å–è®­ç»ƒæ•°æ®ï¼ˆå®Œæ•´çš„ï¼‰
    train = pd.read_csv(raw / "clintox_train.csv")
    print(f"Loaded training data: {train.shape}")
    print(f"Training columns: {train.columns.tolist()}")

    # 2. è¯»å– gold resultsï¼ˆæµ‹è¯•é›†çœŸå®ç­”æ¡ˆï¼‰
    gold_path = Path("/home/aiops/liufan/projects/ScienceAgent-bench/benchmark/eval_programs/gold_results/clintox_gold.csv")

    if not gold_path.exists():
        raise FileNotFoundError(f"Gold results not found: {gold_path}")

    gold = pd.read_csv(gold_path)
    print(f"Loaded gold results: {gold.shape}")
    print(f"Gold columns: {gold.columns.tolist()}")

    # 3. ä¿å­˜è®­ç»ƒæ•°æ®åˆ° publicï¼ˆå®Œæ•´çš„ï¼‰
    train.to_csv(public / "clintox_train.csv", index=False)
    print(f"âœ“ Saved training data to public/")

    # 4. åˆ›å»ºæµ‹è¯•é›†ï¼ˆä»… smilesï¼Œæ— æ ‡ç­¾ï¼‰
    test_public = gold[['smiles']].copy()
    test_public.to_csv(public / "clintox_test.csv", index=False)
    print(f"âœ“ Saved test data (features only) to public/")

    # 5. ä¿å­˜ gold åˆ° privateï¼ˆå®Œæ•´çš„ç­”æ¡ˆï¼‰
    gold.to_csv(private / "clintox_gold.csv", index=False)
    print(f"âœ“ Saved gold results to private/")

    # 6. åˆ›å»º sample_submissionï¼ˆä¸ gold æ ¼å¼ç›¸åŒï¼Œä½†æ ‡ç­¾å¡«é›¶ï¼‰
    sample = gold.copy()
    sample['FDA_APPROVED'] = 0
    sample['CT_TOX'] = 0
    sample.to_csv(public / "sample_submission.csv", index=False)
    print(f"âœ“ Created sample_submission.csv")

    print(f"\\nData preparation completed!")
    print(f"  Public files: {list(public.glob('*'))}")
    print(f"  Private files: {list(private.glob('*'))}")
'''


# ä¿®æ­£åçš„ grade.py
GRADE_PY_FIXED = '''"""
Grading function for ScienceBench task 1 (Clintox)

æ­£ç¡®ç‰ˆæœ¬ï¼šä½¿ç”¨ ROC-AUC è¯„åˆ†ï¼Œä¸ clintox_nn_eval.py ä¸€è‡´
"""

import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score


def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """
    Grade using ROC-AUC score for multi-label classification.

    ä¸ ScienceAgent-bench çš„ clintox_nn_eval.py é€»è¾‘ä¸€è‡´:
    1. æ£€æŸ¥ smiles åˆ—æ˜¯å¦åŒ¹é… (data_correctness)
    2. è®¡ç®— ROC-AUC åˆ†æ•° (func_correctness)
    3. é˜ˆå€¼ 0.77

    Args:
        submission: DataFrame with columns [smiles, FDA_APPROVED, CT_TOX]
        answers: DataFrame with columns [smiles, FDA_APPROVED, CT_TOX]

    Returns:
        float: ROC-AUC score if >= 0.77, else 0.0
    """
    try:
        print(f"Grading Clintox submission...")
        print(f"  Submission shape: {submission.shape}")
        print(f"  Answers shape: {answers.shape}")

        # 1. æ£€æŸ¥ smiles åˆ—æ˜¯å¦åŒ¹é…
        if 'smiles' not in submission.columns or 'smiles' not in answers.columns:
            print("âŒ Missing 'smiles' column")
            return 0.0

        data_correctness = (list(submission["smiles"]) == list(answers["smiles"]))

        if not data_correctness:
            print("âŒ Data correctness failed: smiles columns do not match")
            return 0.0

        print("âœ“ Data correctness passed")

        # 2. æ£€æŸ¥æ ‡ç­¾åˆ—
        required_cols = ['FDA_APPROVED', 'CT_TOX']
        for col in required_cols:
            if col not in submission.columns:
                print(f"âŒ Missing column: {col}")
                return 0.0

        # 3. è®¡ç®— ROC-AUCï¼ˆå¤šæ ‡ç­¾ï¼‰
        try:
            metric = roc_auc_score(
                answers[required_cols],
                submission[required_cols]
            )
        except ValueError as e:
            print(f"âŒ ROC-AUC calculation failed: {e}")
            # å¯èƒ½æ˜¯æ¦‚ç‡å€¼é—®é¢˜ï¼Œå°è¯•è½¬æ¢
            try:
                # å¦‚æœæäº¤çš„æ˜¯ 0/1 æ ‡ç­¾ï¼Œè½¬æ¢ä¸ºæ¦‚ç‡
                metric = roc_auc_score(
                    answers[required_cols],
                    submission[required_cols].astype(float)
                )
            except:
                print("âŒ Could not calculate ROC-AUC")
                return 0.0

        print(f"  ROC-AUC Score: {metric:.4f}")

        # 4. åº”ç”¨é˜ˆå€¼
        threshold = 0.77

        if metric >= threshold:
            print(f"âœ… Func correctness passed (score >= {threshold})")
            return metric
        else:
            print(f"âŒ Func correctness failed (score {metric:.4f} < {threshold})")
            return 0.0

    except Exception as e:
        print(f"âŒ Error in grading: {e}")
        import traceback
        traceback.print_exc()
        return 0.0
'''


# ä¿®æ­£åçš„ config.yaml
CONFIG_YAML_FIXED = '''id: sciencebench-001-clintox-nn
name: "ScienceBench - clintox_nn.py"
competition_type: code
awards_medals: false
prizes: null
description: mlebench.benchmarks.sciencebench.competitions.sciencebench-001-clintox-nn.description:DESCRIPTION

dataset:
  answers: sciencebench-001-clintox-nn/prepared/private/clintox_gold.csv
  sample_submission: sciencebench-001-clintox-nn/prepared/public/sample_submission.csv

grader:
  name: roc_auc
  grade_fn: mlebench.benchmarks.sciencebench.competitions.sciencebench-001-clintox-nn.grade:grade

preparer: mlebench.benchmarks.sciencebench.competitions.sciencebench-001-clintox-nn.prepare:prepare
'''


def main():
    """ä¿®æ­£ clintox ä»»åŠ¡"""
    comp_dir = Path("/home/aiops/liufan/projects/DeepModeling/benchmarks/sciencebench/competitions/sciencebench-001-clintox-nn")

    print("ğŸ”§ Fixing sciencebench-001-clintox-nn...")

    # 1. æ›´æ–° prepare.py
    print("\n1. Updating prepare.py...")
    with open(comp_dir / "prepare.py", "w") as f:
        f.write(PREPARE_PY_FIXED)
    print("   âœ“ prepare.py updated")

    # 2. æ›´æ–° grade.py
    print("\n2. Updating grade.py...")
    with open(comp_dir / "grade.py", "w") as f:
        f.write(GRADE_PY_FIXED)
    print("   âœ“ grade.py updated")

    # 3. æ›´æ–° config.yaml
    print("\n3. Updating config.yaml...")
    with open(comp_dir / "config.yaml", "w") as f:
        f.write(CONFIG_YAML_FIXED)
    print("   âœ“ config.yaml updated")

    print("\nâœ… Fix completed!")
    print("\nNext steps:")
    print("1. Re-run data preparation:")
    print("   python prepare_data.py --competitions sciencebench-001-clintox-nn")
    print("\n2. Verify data structure:")
    print("   ls data/competitions/sciencebench-001-clintox-nn/prepared/private/")
    print("   # Should see: clintox_gold.csv")


if __name__ == '__main__':
    main()
