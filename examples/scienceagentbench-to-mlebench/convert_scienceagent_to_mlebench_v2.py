#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆï¼šæ‰¹é‡è½¬æ¢ ScienceAgent-bench ä»»åŠ¡åˆ° MLE-Bench æ ¼å¼

å…³é”®æ”¹è¿›ï¼š
1. æ­£ç¡®ä½¿ç”¨ eval_programs å’Œ gold_results
2. æ ¹æ® eval è„šæœ¬ç”Ÿæˆæ­£ç¡®çš„è¯„ä¼°é€»è¾‘
3. ä» gold_results è·å–çœŸå®ç­”æ¡ˆ
"""

import pandas as pd
import re
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional


# è·¯å¾„é…ç½®
SCIENCEAGENT_DIR = Path('/home/aiops/liufan/projects/ScienceAgent-bench/benchmark')
SCIENCEBENCH_COMPETITIONS_DIR = Path('/home/aiops/liufan/projects/DeepModeling/benchmarks/sciencebench/competitions')
METADATA_CSV = SCIENCEAGENT_DIR / 'ScienceAgentBench.csv'


def load_scienceagent_metadata() -> pd.DataFrame:
    """åŠ è½½ ScienceAgent-bench çš„å…ƒæ•°æ®"""
    df = pd.read_csv(METADATA_CSV)
    print(f"ğŸ“Š Loaded {len(df)} tasks from ScienceAgentBench.csv")
    return df


def infer_gold_results_path(eval_script_name: str, output_fname: str) -> Optional[Path]:
    """
    æ¨æ–­ gold_results æ–‡ä»¶è·¯å¾„

    ä¾‹å¦‚:
    - eval_script_name: clintox_nn_eval.py
    - å¯èƒ½çš„ gold: clintox_gold.csv
    """
    if pd.isna(eval_script_name):
        return None

    # ç§»é™¤ _eval.py åç¼€
    base_name = eval_script_name.replace('_eval.py', '').replace('.py', '')

    # å¯èƒ½çš„ gold æ–‡ä»¶å
    possible_names = [
        f"{base_name}_gold.csv",
        f"{base_name}_gold.json",
        f"{base_name}_gold.png",
        f"{base_name}.csv",
    ]

    gold_results_dir = SCIENCEAGENT_DIR / 'eval_programs' / 'gold_results'

    for name in possible_names:
        gold_path = gold_results_dir / name
        if gold_path.exists():
            return gold_path

    return None


def analyze_eval_script(eval_script_name: str) -> Dict[str, Any]:
    """
    åˆ†æ eval è„šæœ¬ï¼Œæå–è¯„ä¼°é€»è¾‘

    è¿”å›:
    {
        'metric': 'roc_auc' | 'rmse' | 'visual_similarity' | 'exact_match',
        'threshold': float or None,
        'columns': list of column names
    }
    """
    if pd.isna(eval_script_name):
        return {'metric': 'exact_match', 'threshold': None, 'columns': []}

    eval_path = SCIENCEAGENT_DIR / 'eval_programs' / eval_script_name

    if not eval_path.exists():
        return {'metric': 'exact_match', 'threshold': None, 'columns': []}

    try:
        with open(eval_path, 'r') as f:
            content = f.read()

        # æ£€æµ‹æŒ‡æ ‡ç±»å‹
        if 'roc_auc_score' in content:
            metric = 'roc_auc'
        elif 'mean_squared_error' in content or 'rmse' in content.lower():
            metric = 'rmse'
        elif 'ssim' in content.lower() or 'image' in content.lower():
            metric = 'visual_similarity'
        else:
            metric = 'exact_match'

        # æå–é˜ˆå€¼
        threshold_match = re.search(r'threshold\s*=\s*([\d.]+)', content)
        threshold = float(threshold_match.group(1)) if threshold_match else None

        # æå–åˆ—å
        columns = []
        col_matches = re.findall(r'\[[\'\"]([^\'\"]+)[\'\"]\]', content)
        columns = list(set(col_matches))

        return {'metric': metric, 'threshold': threshold, 'columns': columns}

    except Exception as e:
        print(f"âš  Warning: Could not analyze {eval_script_name}: {e}")
        return {'metric': 'exact_match', 'threshold': None, 'columns': []}


def create_prepare_py_v2(task_data: Dict, dataset_name: str, gold_path: Optional[Path]) -> str:
    """
    ç”Ÿæˆæ”¹è¿›ç‰ˆ prepare.py

    å…³é”®æ”¹è¿›ï¼šä½¿ç”¨ gold_results ä½œä¸ºç­”æ¡ˆ
    """
    output_fname = task_data.get('output_fname', '')
    is_csv = output_fname.endswith('.csv')
    is_image = output_fname.endswith(('.png', '.jpg', '.jpeg'))

    code = f'''"""
Data preparation for ScienceBench task {task_data['instance_id']}

æ”¹è¿›ç‰ˆï¼šä½¿ç”¨ gold_results ä½œä¸ºç­”æ¡ˆ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import shutil
import json


def prepare(raw: Path, public: Path, private: Path):
    """
    Prepare the ScienceAgent task data.

    æ”¹è¿›ï¼šä» gold_results è·å–çœŸå®ç­”æ¡ˆ

    Args:
        raw: Path to ScienceAgent-bench/benchmark/datasets/{dataset_name}
        public: Path to public directory (visible to participants)
        private: Path to private directory (used for grading)
    """
    print(f"Preparing ScienceBench task {task_data['instance_id']}...")

    # Gold results è·¯å¾„
    gold_path = Path("{gold_path if gold_path else 'N/A'}")
'''

    if gold_path and is_csv:
        # CSV æ ¼å¼ï¼šæœ‰ gold_results
        code += f'''

    if not gold_path.exists():
        print(f"âš  Warning: Gold results not found: {{gold_path}}")
        print("Creating placeholder files...")
        create_placeholder_files(public, private, raw)
        return

    # 1. è¯»å– gold resultsï¼ˆæµ‹è¯•é›†ç­”æ¡ˆï¼‰
    gold = pd.read_csv(gold_path)
    print(f"Loaded gold results: {{gold_path}}")
    print(f"Gold shape: {{gold.shape}}")
    print(f"Gold columns: {{gold.columns.tolist()}}")

    # 2. å¤åˆ¶è®­ç»ƒæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    train_files = list(raw.glob('*train*.csv'))
    if train_files:
        train = pd.read_csv(train_files[0])
        train.to_csv(public / train_files[0].name, index=False)
        print(f"Copied training data: {{train_files[0].name}}")

    # 3. åˆ›å»ºæµ‹è¯•é›†ï¼ˆä»…ç‰¹å¾ï¼Œç§»é™¤æ ‡ç­¾åˆ—ï¼‰
    # é€šå¸¸ç¬¬ä¸€åˆ—æ˜¯ ID æˆ–ç‰¹å¾ï¼Œå…¶ä»–åˆ—æ˜¯æ ‡ç­¾
    if len(gold.columns) > 1:
        feature_cols = [gold.columns[0]]  # é€šå¸¸æ˜¯ ID æˆ–ä¸»è¦ç‰¹å¾
        test_public = gold[feature_cols].copy()
    else:
        test_public = gold.copy()

    test_public.to_csv(public / "test_features.csv", index=False)
    print(f"Created test features ({{len(feature_cols)}} columns)")

    # 4. ä¿å­˜å®Œæ•´çš„ gold åˆ° private
    gold.to_csv(private / gold_path.name, index=False)
    print(f"Saved gold results to private/{{gold_path.name}}")

    # 5. åˆ›å»º sample_submissionï¼ˆä¸ gold æ ¼å¼ç›¸åŒï¼Œä½†å¡«å……é›¶ï¼‰
    sample = gold.copy()
    for col in sample.columns:
        if col not in feature_cols:
            # æ ‡ç­¾åˆ—å¡«å……é»˜è®¤å€¼
            if sample[col].dtype in ['int64', 'float64']:
                sample[col] = 0
            else:
                sample[col] = ''

    sample.to_csv(public / "sample_submission.csv", index=False)
    print(f"Created sample_submission.csv")
'''

    elif is_image:
        # å›¾åƒä»»åŠ¡
        code += f'''

    # å›¾åƒä»»åŠ¡ï¼šgold æ˜¯å‚è€ƒå›¾åƒ
    gold_path_obj = Path("{gold_path if gold_path else 'N/A'}")

    if gold_path_obj.exists():
        # å¤åˆ¶å‚è€ƒå›¾åƒåˆ° private
        shutil.copy2(gold_path_obj, private / gold_path_obj.name)
        print(f"Copied reference image to private/{{gold_path_obj.name}}")

    # å¤åˆ¶æ•°æ®æ–‡ä»¶åˆ° public
    for file in raw.rglob('*'):
        if file.is_file() and not file.name.startswith('.'):
            rel_path = file.relative_to(raw)
            target = public / rel_path
            target.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(file, target)
            print(f"  Copied: {{rel_path}}")

    # åˆ›å»ºæäº¤è¯´æ˜
    with open(public / "README.txt", "w") as f:
        f.write(f"Task: Generate image {{gold_path_obj.name}}\\n")
        f.write(f"Expected output: {{gold_path_obj.name}}\\n")
'''

    else:
        # å…¶ä»–æ ¼å¼
        code += f'''

    # å¤åˆ¶æ‰€æœ‰æ•°æ®åˆ° public
    for file in raw.rglob('*'):
        if file.is_file() and not file.name.startswith('.'):
            rel_path = file.relative_to(raw)
            target = public / rel_path
            target.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(file, target)
            print(f"  Copied: {{rel_path}}")

    # åˆ›å»ºé»˜è®¤çš„æäº¤æ–‡ä»¶
    with open(public / "sample_submission.txt", "w") as f:
        f.write("Your submission should match the format specified in the task description.\\n")
'''

    code += '''

    print(f"\\nData preparation completed!")
    print(f"  Public files: {list(public.glob('*'))}")
    print(f"  Private files: {list(private.glob('*'))}")


def create_placeholder_files(public: Path, private: Path, raw: Path):
    """åˆ›å»ºå ä½ç¬¦æ–‡ä»¶ï¼ˆå½“ gold ä¸å­˜åœ¨æ—¶ï¼‰"""
    # å°è¯•å¤åˆ¶åŸå§‹æ•°æ®
    for file in raw.rglob('*.csv'):
        if file.is_file():
            shutil.copy2(file, public / file.name)

    # åˆ›å»ºé»˜è®¤æ–‡ä»¶
    pd.DataFrame({"info": ["Data not available"]}).to_csv(
        public / "sample_submission.csv", index=False
    )
    pd.DataFrame({"info": ["Answer not available"]}).to_csv(
        private / "answer.csv", index=False
    )
'''

    return code


def create_grade_py_v2(task_data: Dict, eval_info: Dict) -> str:
    """
    ç”Ÿæˆæ”¹è¿›ç‰ˆ grade.py

    æ ¹æ® eval_info ç”Ÿæˆç›¸åº”çš„è¯„åˆ†é€»è¾‘
    """
    metric = eval_info['metric']
    threshold = eval_info['threshold']

    code = f'''"""
Grading function for ScienceBench task {task_data['instance_id']}

Metric: {metric}
Threshold: {threshold if threshold else 'N/A'}
"""

import pandas as pd
import numpy as np
from pathlib import Path
'''

    if metric == 'roc_auc':
        code += '''from sklearn.metrics import roc_auc_score


def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """
    Grade using ROC-AUC score (multi-label)

    Similar to ScienceAgent-bench eval logic
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒçš„ ID åˆ—ï¼ˆå¦‚ smilesï¼‰
        id_col = submission.columns[0]  # é€šå¸¸ç¬¬ä¸€åˆ—æ˜¯ ID

        # æ£€æŸ¥ ID åŒ¹é…
        if id_col in answers.columns:
            if not (list(submission[id_col]) == list(answers[id_col])):
                print("âš  Warning: ID columns do not match")
                return 0.0

        # è·å–æ ‡ç­¾åˆ—ï¼ˆé™¤äº†ç¬¬ä¸€åˆ—ï¼‰
        label_cols = [col for col in answers.columns if col != id_col]

        if not label_cols:
            print("âš  Warning: No label columns found")
            return 0.0

        # è®¡ç®— ROC-AUC
        metric = roc_auc_score(
            answers[label_cols],
            submission[label_cols]
        )

        print(f"ROC-AUC Score: {metric:.4f}")
'''

        if threshold:
            code += f'''
        # åº”ç”¨é˜ˆå€¼
        threshold = {threshold}
        if metric >= threshold:
            return metric
        else:
            print(f"Score {{metric:.4f}} below threshold {{threshold}}")
            return 0.0
'''
        else:
            code += '''
        return metric
'''

    elif metric == 'rmse':
        code += '''from sklearn.metrics import mean_squared_error


def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """
    Grade using RMSE (lower is better, return negative for consistency)
    """
    try:
        # å¯¹é½æ•°æ®
        id_col = submission.columns[0]
        label_cols = [col for col in answers.columns if col != id_col]

        # è®¡ç®— RMSE
        rmse = np.sqrt(mean_squared_error(
            answers[label_cols],
            submission[label_cols]
        ))

        print(f"RMSE: {rmse:.4f}")

        # è¿”å›è´Ÿå€¼ï¼ˆæ›´é«˜çš„åˆ†æ•°æ›´å¥½ï¼‰
        return -rmse
'''

    elif metric == 'visual_similarity':
        code += '''from PIL import Image
import imagehash


def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """
    Grade by comparing images

    Note: This is a placeholder. Actual implementation
    would need access to the image files.
    """
    # å›¾åƒè¯„åˆ†éœ€è¦è®¿é—®å®é™…çš„å›¾åƒæ–‡ä»¶
    # è¿™é‡Œè¿”å›å ä½ç¬¦åˆ†æ•°
    print("âš  Warning: Image grading not fully implemented")
    return 0.5
'''

    else:  # exact_match
        code += '''

def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """
    Grade using exact match
    """
    # æ£€æŸ¥å½¢çŠ¶
    if submission.shape != answers.shape:
        print(f"Shape mismatch: {submission.shape} vs {answers.shape}")
        return 0.0

    # è®¡ç®—åŒ¹é…æ¯”ä¾‹
    matches = (submission.values == answers.values).sum()
    total = submission.size

    return matches / total if total > 0 else 0.0
'''

    code += '''

    except Exception as e:
        print(f"Error in grading: {e}")
        import traceback
        traceback.print_exc()
        return 0.0
'''

    return code


# ... (å…¶ä»–å‡½æ•°ä¿æŒä¸å˜ï¼Œä½†ä½¿ç”¨æ–°çš„ create_prepare_py_v2 å’Œ create_grade_py_v2)

# æ­¤å¤„çœç•¥å…¶ä»–è¾…åŠ©å‡½æ•°ï¼Œå®ƒä»¬ä¸ v1 ç‰ˆæœ¬ç›¸åŒ
# åªéœ€è¦åœ¨ convert_task å‡½æ•°ä¸­è°ƒç”¨æ–°çš„ç”Ÿæˆå‡½æ•°
