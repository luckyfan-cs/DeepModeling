#!/usr/bin/env python3
"""
æ‰¹é‡è½¬æ¢ ScienceAgent-bench ä»»åŠ¡åˆ° MLE-Bench æ ¼å¼

ä½¿ç”¨æ–¹æ³•:
    python convert_scienceagent_to_mlebench.py --instance-ids 1 2 3
    python convert_scienceagent_to_mlebench.py --all
    python convert_scienceagent_to_mlebench.py --list
"""

import argparse
import json
import re
import subprocess
import textwrap
from string import Template
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd


# è·¯å¾„é…ç½®
SCIENCEAGENT_DIR = Path('/home/aiops/liufan/projects/ScienceAgent-bench/benchmark')
SCIENCEBENCH_REGISTRY_DIR = Path('/home/aiops/liufan/projects/DeepModeling/benchmarks/sciencebench/competitions')
SCIENCEBENCH_DATA_DIR = Path('/home/aiops/liufan/projects/ScienceAgent-bench/competitions')
GOLD_RESULTS_DIR = SCIENCEAGENT_DIR / 'eval_programs' / 'gold_results'

IMAGE_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.tif', '.tiff'}
METADATA_CSV = SCIENCEAGENT_DIR / 'ScienceAgentBench.csv'


def load_scienceagent_metadata() -> pd.DataFrame:
    """åŠ è½½ ScienceAgent-bench çš„å…ƒæ•°æ®"""
    df = pd.read_csv(METADATA_CSV)
    print(f"ğŸ“Š Loaded {len(df)} tasks from ScienceAgentBench.csv")
    return df


def clean_task_instruction(task_inst: str) -> str:
    """æ¸…ç†ä»»åŠ¡è¯´æ˜ï¼Œç§»é™¤è·¯å¾„ä»¥åŠå…·ä½“æ¨¡å‹è¦æ±‚"""
    if pd.isna(task_inst) or not str(task_inst).strip():
        return "Generate the requested output based on the provided scientific data."

    text = str(task_inst)

    # æ ‡å‡†åŒ–ç©ºç™½å’Œæ¢è¡Œ
    text = re.sub(r"\s+", " ", text.strip())

    # æ›¿æ¢ pred_results ç›¸å…³çš„è·¯å¾„å¼•ç”¨
    text = re.sub(r'"[^"\n]*pred_results[^"\n]*"', '"output file"', text, flags=re.IGNORECASE)
    text = re.sub(r'`[^`\n]*pred_results[^`\n]*`', '`output file`', text, flags=re.IGNORECASE)

    sentences = re.split(r'(?<=[.!?])\s+', text)
    cleaned_sentences: List[str] = []
    for sentence in sentences:
        lowered = sentence.lower()

        # ç§»é™¤ä¿å­˜è·¯å¾„ç›¸å…³å†…å®¹
        if any(keyword in lowered for keyword in ['pred_results', ' save ', 'saving to', 'stored in', '/tmp/', './']):
            continue

        # ç§»é™¤å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šæ¨¡å‹æˆ–åº“çš„å¥å­
        if 'use ' in lowered and (' model' in lowered or '`' in sentence):
            continue

        cleaned_sentences.append(sentence.strip())

    if not cleaned_sentences:
        return "Generate the requested output based on the provided scientific data."

    cleaned = ' '.join(cleaned_sentences)
    cleaned = re.sub(r"\s+", " ", cleaned).strip()
    return cleaned


def is_image_output(output_fname: Optional[str]) -> bool:
    if not output_fname or pd.isna(output_fname):
        return False
    return Path(str(output_fname)).suffix.lower() in IMAGE_EXTENSIONS


def generate_candidate_tokens(task_data: Dict[str, Any]) -> List[str]:
    tokens: List[str] = []

    output_fname = task_data.get('output_fname')
    if isinstance(output_fname, str) and output_fname:
        stem = Path(output_fname).stem.lower()
        tokens.append(stem)
        tokens.append(stem.replace('pred_', '').replace('_pred', '').replace('-pred', ''))

    for key in ['gold_program_name', 'eval_script_name']:
        value = task_data.get(key)
        if isinstance(value, str) and value:
            stem = Path(value).stem.lower()
            tokens.append(stem)
            tokens.append(stem.replace('_eval', ''))

    dataset_tree = task_data.get('dataset_folder_tree')
    if isinstance(dataset_tree, str):
        match = re.search(r'\|--\s*([^/\n]+)', dataset_tree)
        if match:
            tokens.append(match.group(1).strip().lower())

    return [token for token in {t for t in tokens if t}]


def find_gold_result_path(task_data: Dict[str, Any]) -> Optional[Path]:
    """æ ¹æ®ä»»åŠ¡ä¿¡æ¯çŒœæµ‹ gold_results æ–‡ä»¶è·¯å¾„"""
    if not GOLD_RESULTS_DIR.exists():
        return None

    candidates = generate_candidate_tokens(task_data)
    if not candidates:
        return None

    best_match: Optional[Path] = None
    best_score = 0
    expected_suffix = None

    output_fname = task_data.get('output_fname')
    if isinstance(output_fname, str) and output_fname:
        expected_suffix = Path(output_fname).suffix.lower()

    for gold_file in GOLD_RESULTS_DIR.iterdir():
        if not gold_file.is_file():
            continue

        stem = gold_file.stem.lower()
        score = 0

        for token in candidates:
            if token in stem or stem in token:
                score = max(score, len(token))

        if expected_suffix and gold_file.suffix.lower() == expected_suffix:
            score += 5

        if score > best_score:
            best_match = gold_file
            best_score = score

    return best_match


def create_competition_id(instance_id: int, gold_program_name: str) -> str:
    """ç”Ÿæˆæ¯”èµ› ID"""
    # ä» gold_program_name æå–åŸºæœ¬åç§°
    if pd.isna(gold_program_name):
        name_part = f"task-{instance_id}"
    else:
        # ç§»é™¤ .py åç¼€
        name_part = gold_program_name.replace('.py', '')
        # è½¬æ¢ä¸ºå°å†™ï¼Œç”¨è¿å­—ç¬¦åˆ†éš”
        name_part = re.sub(r'[_\s]+', '-', name_part.lower())
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        name_part = re.sub(r'[^a-z0-9-]', '', name_part)

    return f"sciencebench-{instance_id:03d}-{name_part}"


def extract_dataset_name(dataset_folder_tree: str) -> str:
    """ä»æ•°æ®é›†æ–‡ä»¶å¤¹æ ‘ä¸­æå–æ•°æ®é›†åç§°"""
    if pd.isna(dataset_folder_tree):
        return "unknown"

    # æå–ç¬¬ä¸€ä¸ªé¡¶å±‚ç›®å½•å
    # ä¾‹å¦‚ "|-- clintox/\n|---- ..." -> "clintox"
    match = re.search(r'\|--\s*([^/\n]+)', dataset_folder_tree)
    if match:
        return match.group(1).strip()
    return "unknown"


def infer_metric_from_task(task_inst: str, output_fname: str, subtask_categories: str) -> str:
    """æ ¹æ®ä»»åŠ¡ç±»å‹æ¨æ–­è¯„ä¼°æŒ‡æ ‡"""
    task_lower = (task_inst or "").lower()
    output_lower = (output_fname or "").lower()
    subtask_lower = (subtask_categories or "").lower()

    # å¯è§†åŒ–ä»»åŠ¡
    if any(keyword in task_lower for keyword in ['visualize', 'plot', 'figure', 'chart']):
        return 'visual_similarity'
    if any(keyword in output_lower for keyword in ['.png', '.jpg', '.jpeg', '.pdf']):
        return 'visual_similarity'

    # æ·±åº¦å­¦ä¹ /æœºå™¨å­¦ä¹ ä»»åŠ¡
    if 'deep learning' in subtask_lower or 'machine learning' in subtask_lower:
        if 'classification' in task_lower or 'predict' in task_lower:
            return 'accuracy'
        elif 'regression' in task_lower:
            return 'rmse'

    # å›å½’ä»»åŠ¡
    if any(keyword in task_lower for keyword in ['regression', 'predict.*values']):
        return 'rmse'

    # åˆ†ç±»ä»»åŠ¡
    if any(keyword in task_lower for keyword in ['classification', 'classify', 'toxicity']):
        return 'accuracy'

    # ç‰¹å¾é€‰æ‹©/å·¥ç¨‹
    if 'feature' in subtask_lower:
        return 'exact_match'

    # é»˜è®¤ä½¿ç”¨ RMSEï¼ˆé€‚ç”¨äºå¤§å¤šæ•°æ•°å€¼é¢„æµ‹ï¼‰
    return 'rmse'


def create_image_prepare_py(task_data: Dict, dataset_name: str, gold_path: Optional[Path]) -> str:
    """ç”Ÿæˆå›¾åƒä»»åŠ¡çš„ prepare.py"""
    output_fname = task_data.get('output_fname', 'output.png') or 'output.png'
    expected_filename = Path(output_fname).name
    gold_literal = gold_path.as_posix() if gold_path else ''

    template = '''"""
Data preparation for ScienceBench task {instance_id}
Dataset: {dataset_name}
"""

import base64
from pathlib import Path
import shutil
import pandas as pd


EXPECTED_FILENAME = "{expected_filename}"
GOLD_IMAGE_PATH = Path("{gold_literal}") if "{gold_literal}" else None
SOURCE_DATASET = "{dataset_name}"


def prepare(raw: Path, public: Path, private: Path):
    """Prepare data for image-based ScienceBench task."""
    print("=" * 60)
    print("Preparing ScienceBench Task {instance_id}")
    print("Dataset:", SOURCE_DATASET)
    print("=" * 60)
    print("Raw directory:", raw)
    print("Public directory:", public)
    print("Private directory:", private)

    if not raw.exists():
        print("\\nâš  Warning: Raw data directory not found:", raw)
        public.mkdir(parents=True, exist_ok=True)
        private.mkdir(parents=True, exist_ok=True)
        placeholder = pd.DataFrame([
            {{"file_name": EXPECTED_FILENAME, "image_base64": ""}}
        ])
        placeholder.to_csv(public / "sample_submission.csv", index=False)
        placeholder.to_csv(private / "answer.csv", index=False)
        return

    file_count = 0
    for file in raw.rglob('*'):
        if file.is_file() and not file.name.startswith('.'):
            rel_path = file.relative_to(raw)
            target = public / rel_path
            target.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(file, target)
            file_count += 1
            if file_count <= 10:
                print("  âœ“ Copied:", rel_path)

    if file_count > 10:
        print("  ... and", file_count - 10, "more files")
    print("  Total files copied:", file_count)

    public.mkdir(parents=True, exist_ok=True)
    private.mkdir(parents=True, exist_ok=True)

    gold_base64 = ""
    if GOLD_IMAGE_PATH and GOLD_IMAGE_PATH.exists():
        gold_bytes = GOLD_IMAGE_PATH.read_bytes()
        (private / EXPECTED_FILENAME).write_bytes(gold_bytes)
        gold_base64 = base64.b64encode(gold_bytes).decode("utf-8")
        print("âœ“ Embedded gold image from", GOLD_IMAGE_PATH)
    else:
        print("âš  Gold image not found; creating empty placeholder.")

    sample_df = pd.DataFrame([
        {{"file_name": EXPECTED_FILENAME, "image_base64": ""}}
    ])
    sample_df.to_csv(public / "sample_submission.csv", index=False)
    print("âœ“ Created sample_submission.csv")

    answer_df = pd.DataFrame([
        {{"file_name": EXPECTED_FILENAME, "image_base64": gold_base64}}
    ])
    answer_df.to_csv(private / "answer.csv", index=False)
    print("âœ“ Created answer.csv with encoded gold image")

    print("\\nData preparation completed!")
'''

    return template.format(
        instance_id=task_data['instance_id'],
        dataset_name=dataset_name,
        expected_filename=expected_filename,
        gold_literal=gold_literal,
    )


def create_image_grade_py(task_data: Dict) -> str:
    """ç”Ÿæˆå›¾åƒä»»åŠ¡çš„ grade.py"""
    expected_filename = Path(task_data.get('output_fname', 'output.png') or 'output.png').name

    template = '''"""
Grading function for ScienceBench task {instance_id}

Image similarity is approximated via pixel-level comparison after decoding
base64 encoded submissions.
"""

import base64
import io
import numpy as np
import pandas as pd
from PIL import Image


EXPECTED_FILENAME = "{expected_filename}"


def _decode_image(data: str) -> Image.Image:
    if not isinstance(data, str) or not data.strip():
        raise ValueError("Empty image_base64 value")
    buffer = io.BytesIO(base64.b64decode(data))
    return Image.open(buffer).convert("RGB")


def _similarity_score(gold_img: Image.Image, pred_img: Image.Image) -> float:
    pred_resized = pred_img.resize(gold_img.size)
    gold_arr = np.asarray(gold_img, dtype=np.float32) / 255.0
    pred_arr = np.asarray(pred_resized, dtype=np.float32) / 255.0

    mse = float(np.mean((gold_arr - pred_arr) ** 2))
    return max(0.0, 1.0 - mse)


def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    required_columns = {{"file_name", "image_base64"}}
    if not required_columns.issubset(submission.columns):
        raise ValueError(f"Submission must contain columns: {{required_columns}}")

    if not required_columns.issubset(answers.columns):
        raise ValueError(f"Answers must contain columns: {{required_columns}}")

    merged = pd.merge(
        answers.rename(columns={{"image_base64": "image_base64_gold"}}),
        submission.rename(columns={{"image_base64": "image_base64_pred"}}),
        on="file_name",
        how="inner",
    )

    if merged.empty:
        return 0.0

    scores = []
    for _, row in merged.iterrows():
        try:
            gold_img = _decode_image(row["image_base64_gold"])
            pred_img = _decode_image(row["image_base64_pred"])
        except ValueError:
            scores.append(0.0)
            continue

        scores.append(_similarity_score(gold_img, pred_img))

    return float(np.mean(scores)) if scores else 0.0
'''

    return template.format(
        instance_id=task_data['instance_id'],
        expected_filename=expected_filename,
    )


def create_config_yaml(comp_id: str, task_name: str, metric: str) -> str:
    """ç”Ÿæˆ config.yaml å†…å®¹"""
    return f"""id: {comp_id}
name: "ScienceBench - {task_name}"
competition_type: code
awards_medals: false
prizes: null
description: benchmarks/sciencebench/competitions/{comp_id}/description.md

dataset:
  answers: {comp_id}/prepared/private/answer.csv
  sample_submission: {comp_id}/prepared/public/sample_submission.csv

grader:
  name: {metric}
  grade_fn: benchmarks.sciencebench.competitions.{comp_id}.grade:grade

preparer: benchmarks.sciencebench.competitions.{comp_id}.prepare:prepare
"""


def create_description_md(task_data: Dict, task_type: str, metric: str) -> str:
    """ç”Ÿæˆ description.md å†…å®¹"""
    instance_id = task_data['instance_id']
    domain = task_data.get('domain', 'Unknown')
    subtask = task_data.get('subtask_categories', 'Unknown')
    github = task_data.get('github_name', '')
    task_inst = clean_task_instruction(task_data.get('task_inst', ''))
    dataset_preview = task_data.get('dataset_preview', '')
    expected_output = Path(task_data.get('output_fname', 'output')).name

    overview_lines = [
        f"- Domain: {domain}",
        f"- Subtask Categories: {subtask}",
        f"- Source: {github if github else 'N/A'}",
        f"- Expected Output: {expected_output}",
        f"- Output Type: {task_type.replace('_', ' ').title()}",
    ]

    dataset_section = dataset_preview if dataset_preview and not pd.isna(dataset_preview) else 'N/A'

    if task_type == 'image':
        submission_text = (
            "Submit `sample_submission.csv` with the columns `file_name` and `image_base64`. "
            "Encode your final image as base64 (UTF-8 string) and associate it with the expected file name."
        )
        evaluation_text = (
            "The grader decodes your base64 image, rescales it to the reference size, "
            "and computes a similarity score between 0 and 1."
        )
    else:
        submission_text = (
            "Submit `sample_submission.csv` with the same header and column order as the template. "
            "Ensure numeric columns retain their dtype and identifiers remain aligned."
        )

        metric_descriptions = {
            'roc_auc': 'Receiver Operating Characteristic AUC (higher is better).',
            'accuracy': 'Classification accuracy (higher is better).',
            'rmse': 'Negative root mean squared error (closer to zero is better).',
            'mae': 'Negative mean absolute error (closer to zero is better).',
            'exact_match': 'Element-wise equality between your submission and the reference.',
            'visual_similarity': 'Similarity score computed between generated visual artifacts.',
        }
        evaluation_text = metric_descriptions.get(metric, 'Comparison with the reference answer file.')

    description = f"""# ScienceBench Task {instance_id}

## Overview

{chr(10).join(overview_lines)}

## Task

{task_inst}

## Dataset

{dataset_section}

## Submission Format

{submission_text}

## Evaluation

{evaluation_text}
"""

    return description





def create_prepare_py(task_data: Dict, dataset_name: str, gold_path: Optional[Path]) -> str:
    """ç”Ÿæˆ prepare.py å†…å®¹ï¼Œä½¿ç”¨çœŸå® gold ç»“æœ"""
    dataset_literal = dataset_name or 'unknown'
    gold_literal = gold_path.as_posix() if gold_path else ''
    task_id = task_data['instance_id']

    template = Template(
        """
"""
Data preparation for ScienceBench task $TASK_ID
Dataset: $DATASET
"""

import json
import shutil
from pathlib import Path

import numpy as np
import pandas as pd


SOURCE_DATASET = "$DATASET"
GOLD_PATH = Path("$GOLD_PATH") if "$GOLD_PATH" else None
ANSWER_FILENAME = "answer.csv"
SAMPLE_FILENAME = "sample_submission.csv"


def load_gold_dataframe(path: Path) -> pd.DataFrame:
    suffix = path.suffix.lower()

    if suffix == ".csv":
        return pd.read_csv(path)

    if suffix in {".json", ".jsonl"}:
        if suffix == ".jsonl":
            records = []
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        records.append(json.loads(line))
        else:
            with open(path, "r", encoding="utf-8") as f:
                payload = json.load(f)
            if isinstance(payload, list):
                records = payload
            elif isinstance(payload, dict):
                records = [payload]
            else:
                records = [{"value": payload}]

        return pd.json_normalize(records)

    if suffix in {".pkl", ".pickle"}:
        obj = pd.read_pickle(path)
        if isinstance(obj, pd.DataFrame):
            return obj.reset_index(drop=True)
        if isinstance(obj, dict):
            return pd.json_normalize(obj)
        if isinstance(obj, list):
            return pd.json_normalize(obj)
        return pd.DataFrame({"value": [obj]})

    if suffix in {".npy", ".npz"}:
        arr = np.load(path, allow_pickle=True)
        if isinstance(arr, np.ndarray):
            if arr.dtype.names is not None:
                return pd.DataFrame(arr.tolist())
            if arr.ndim == 1:
                return pd.DataFrame({"value": arr.tolist()})
            reshaped = arr.reshape(arr.shape[0], -1) if arr.ndim > 2 else arr
            return pd.DataFrame(reshaped)
        if isinstance(arr, dict):
            return pd.DataFrame(arr)
        return pd.DataFrame({"value": [arr]})

    if suffix in {".txt", ".tsv"}:
        if suffix == ".tsv":
            return pd.read_csv(path, sep="	")
        text = path.read_text(encoding="utf-8").splitlines()
        return pd.DataFrame({"value": text})

    if suffix in {".xlsx", ".xls"}:
        return pd.read_excel(path)

    raise ValueError(f"Unsupported gold result format: {suffix}")


def create_sample_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    sample = df.copy()
    for column in sample.columns:
        if pd.api.types.is_numeric_dtype(sample[column]):
            sample[column] = 0
        else:
            sample[column] = ""
    return sample.fillna("")


def ensure_directory(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def create_placeholder_files(public: Path, private: Path) -> None:
    ensure_directory(public)
    ensure_directory(private)

    pd.DataFrame({"info": ["Data not available"]}).to_csv(
        public / SAMPLE_FILENAME, index=False
    )

    pd.DataFrame({"info": ["Answer not available"]}).to_csv(
        private / ANSWER_FILENAME, index=False
    )

    print("Placeholder files created")


def prepare(raw: Path, public: Path, private: Path):
    """Prepare the ScienceAgent task data."""
    print("=" * 60)
    print("Preparing ScienceBench Task $TASK_ID")
    print("Dataset:", SOURCE_DATASET)
    print("=" * 60)
    print("Raw directory:", raw)
    print("Public directory:", public)
    print("Private directory:", private)

    ensure_directory(public)
    ensure_directory(private)

    if raw.exists():
        print("\nCopying data files to public directory...")
        file_count = 0
        for file in raw.rglob('*'):
            if file.is_file() and not file.name.startswith('.'):
                rel_path = file.relative_to(raw)
                target = public / rel_path
                target.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(file, target)
                file_count += 1
                if file_count <= 10:
                    print("  âœ“ Copied:", rel_path)
        if file_count > 10:
            print("  ... and", file_count - 10, "more files")
        print("  Total files copied:", file_count)
    else:
        print("\nâš  Warning: Raw data directory not found:", raw)

    if GOLD_PATH and GOLD_PATH.exists():
        try:
            gold_df = load_gold_dataframe(GOLD_PATH)
            sample_df = create_sample_dataframe(gold_df)

            answer_path = private / ANSWER_FILENAME
            sample_path = public / SAMPLE_FILENAME

            gold_df.to_csv(answer_path, index=False)
            sample_df.to_csv(sample_path, index=False)

            print("âœ“ Created answer file:", answer_path)
            print("âœ“ Created sample submission:", sample_path)

            gold_copy = private / GOLD_PATH.name
            if GOLD_PATH != gold_copy:
                shutil.copy2(GOLD_PATH, gold_copy)
                print("âœ“ Copied original gold file:", gold_copy)

        except Exception as exc:
            print("âš  Failed to process gold results:", exc)
            print("   Falling back to placeholder files")
            create_placeholder_files(public, private)
    else:
        print("âš  Gold results not found; creating placeholder files")
        create_placeholder_files(public, private)

    print("\nData preparation completed!")
    public_list = [p.name for p in public.iterdir() if p.is_file()]
    private_list = [p.name for p in private.iterdir() if p.is_file()]
    print("  Public files:", public_list)
    print("  Private files:", private_list)
"""
    )

    script = template.substitute(
        TASK_ID=task_id,
        DATASET=dataset_literal,
        GOLD_PATH=gold_literal,
    )

    return textwrap.dedent(script)
def create_grade_py(task_data: Dict, metric: str) -> str:
    """ç”Ÿæˆ grade.py å†…å®¹"""
    output_fname = task_data.get('output_fname', 'output.csv')
    is_image = output_fname.endswith(('.png', '.jpg', '.jpeg', '.pdf'))

    code = f'''"""
Grading function for ScienceBench task {task_data['instance_id']}
"""

import pandas as pd
import numpy as np
from pathlib import Path
'''

    if metric == 'accuracy':
        code += '''from sklearn.metrics import accuracy_score


def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """
    Grade submission using accuracy metric.

    Args:
        submission: DataFrame with predictions
        answers: DataFrame with ground truth

    Returns:
        Accuracy score (0-1)
    """
    # å¯¹é½æ•°æ®
    if 'id' in submission.columns and 'id' in answers.columns:
        merged = pd.merge(answers, submission, on='id', suffixes=('_true', '_pred'))

        # æ‰¾åˆ°é¢„æµ‹åˆ—
        pred_col = [c for c in merged.columns if c.endswith('_pred')][0]
        true_col = pred_col.replace('_pred', '_true')

        return accuracy_score(merged[true_col], merged[pred_col])
    else:
        # ç®€å•æ¯”è¾ƒ
        return float(np.mean(submission.values == answers.values))
'''

    elif metric == 'rmse':
        code += '''from sklearn.metrics import mean_squared_error


def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """
    Grade submission using RMSE metric (lower is better).

    Args:
        submission: DataFrame with predictions
        answers: DataFrame with ground truth

    Returns:
        Negative RMSE (higher is better for consistency)
    """
    # å¯¹é½æ•°æ®
    if 'id' in submission.columns and 'id' in answers.columns:
        merged = pd.merge(answers, submission, on='id', suffixes=('_true', '_pred'))

        # æ‰¾åˆ°é¢„æµ‹åˆ—
        pred_col = [c for c in merged.columns if c.endswith('_pred')][0]
        true_col = pred_col.replace('_pred', '_true')

        rmse = mean_squared_error(merged[true_col], merged[pred_col], squared=False)
        return -rmse  # è´Ÿæ•°ï¼Œå› ä¸ºæ›´é«˜çš„åˆ†æ•°æ›´å¥½
    else:
        # ç®€å• RMSE
        rmse = np.sqrt(np.mean((submission.values - answers.values) ** 2))
        return -rmse
'''

    elif metric == 'visual_similarity':
        code += '''from PIL import Image
import imagehash


def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """
    Grade submission by comparing images.

    Args:
        submission: DataFrame or path info about submitted image
        answers: DataFrame or path info about reference image

    Returns:
        Similarity score (0-1)
    """
    # This is a placeholder implementation
    # Actual image comparison would require the image files

    # å¦‚æœæäº¤çš„æ˜¯æ–‡ä»¶è·¯å¾„æˆ–å…ƒæ•°æ®ï¼Œæ¯”è¾ƒå®ƒä»¬
    try:
        # å°è¯•è®¡ç®—å›¾åƒå“ˆå¸Œè·ç¦»
        # å®é™…å®ç°éœ€è¦è®¿é—®å›¾åƒæ–‡ä»¶
        return 0.5  # Placeholder
    except Exception as e:
        print(f"Error in visual comparison: {e}")
        return 0.0
'''

    else:  # exact_match
        code += '''

def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """
    Grade submission using exact match.

    Args:
        submission: DataFrame with predictions
        answers: DataFrame with ground truth

    Returns:
        Match ratio (0-1)
    """
    # é€å…ƒç´ æ¯”è¾ƒ
    if submission.shape != answers.shape:
        print(f"Shape mismatch: submission {submission.shape} vs answers {answers.shape}")
        return 0.0

    # è®¡ç®—åŒ¹é…æ¯”ä¾‹
    matches = (submission.values == answers.values).sum()
    total = submission.size

    return matches / total if total > 0 else 0.0
'''

    return code


def create_leaderboard_csv() -> str:
    """ç”Ÿæˆ leaderboard.csv å†…å®¹"""
    return """submission_id,score,username,date
random_baseline,0.0,random,2024-01-01
"""


def create_checksums_yaml() -> str:
    """ç”Ÿæˆ checksums.yaml å†…å®¹"""
    return """# Checksums for data integrity verification
# Will be populated after data preparation

public: {}
private: {}
"""


def convert_task(instance_id: int, task_data: Dict, dry_run: bool = False) -> bool:
    """
    è½¬æ¢å•ä¸ªä»»åŠ¡

    Args:
        instance_id: ä»»åŠ¡ ID
        task_data: ä»»åŠ¡æ•°æ®å­—å…¸
        dry_run: æ˜¯å¦åªé¢„è§ˆä¸å®é™…åˆ›å»º

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        # ç”Ÿæˆ competition ID
        comp_id = create_competition_id(instance_id, task_data.get('gold_program_name'))

        print(f"\n{'='*60}")
        print(f"Converting Task {instance_id} -> {comp_id}")
        print(f"{'='*60}")
        print(f"Domain: {task_data.get('domain', 'Unknown')}")
        print(f"Subtask: {task_data.get('subtask_categories', 'Unknown')}")

        if dry_run:
            print("ğŸ” DRY RUN - No files will be created")
            return True

        # åˆ›å»ºæ³¨å†Œç›®å½•ï¼ˆregistry - å­˜æ”¾ config, description, grade, prepare æ–‡ä»¶ï¼‰
        comp_dir = SCIENCEBENCH_REGISTRY_DIR / comp_id
        comp_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ“ Created registry directory: {comp_dir}")

        # åˆ›å»ºæ•°æ®ç›®å½•ï¼ˆdata-dir - å­˜æ”¾ prepared/public å’Œ prepared/privateï¼‰
        data_dir = SCIENCEBENCH_DATA_DIR / comp_id
        data_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ“ Created data directory: {data_dir}")

        # æå–æ•°æ®é›†åç§°
        dataset_name = extract_dataset_name(task_data.get('dataset_folder_tree'))

        output_fname = task_data.get('output_fname', '')
        task_type = 'image' if is_image_output(output_fname) else 'tabular'
        gold_path = find_gold_result_path(task_data)

        if gold_path:
            print(f"âœ“ Located gold reference: {gold_path.name}")
        else:
            print("âš  Gold reference not found in gold_results directory")

        # æ¨æ–­è¯„ä¼°æŒ‡æ ‡
        metric = infer_metric_from_task(
            task_data.get('task_inst'),
            output_fname,
            task_data.get('subtask_categories')
        )

        if task_type == 'image':
            metric = 'image_similarity'

        print(f"âœ“ Inferred metric: {metric}")

        description_content = create_description_md(task_data, task_type, metric)

        if task_type == 'image':
            prepare_code = create_image_prepare_py(task_data, dataset_name, gold_path)
            grade_code = create_image_grade_py(task_data)
        else:
            prepare_code = create_prepare_py(task_data, dataset_name, gold_path)
            grade_code = create_grade_py(task_data, metric)

        # ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶
        files_to_create = {
            'config.yaml': create_config_yaml(
                comp_id,
                task_data.get('gold_program_name', f'Task {instance_id}'),
                metric
            ),
            'description.md': description_content,
            'prepare.py': prepare_code,
            'grade.py': grade_code,
            'leaderboard.csv': create_leaderboard_csv(),
            'checksums.yaml': create_checksums_yaml()
        }

        for filename, content in files_to_create.items():
            file_path = comp_dir / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"âœ“ Created {filename}")

        print(f"âœ… Task {instance_id} converted successfully!")
        return True

    except Exception as e:
        print(f"âŒ Error converting task {instance_id}: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_auto_prepare(comp_id: str, dataset_name: str, timeout: int = 120) -> bool:
    """
    è‡ªåŠ¨è¿è¡Œ prepare.py

    Args:
        comp_id: Competition ID
        dataset_name: Dataset name (for raw data path)
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    comp_dir = SCIENCEBENCH_REGISTRY_DIR / comp_id
    prepare_script = comp_dir / "prepare.py"

    if not prepare_script.exists():
        print(f"âš  Prepare script not found: {prepare_script}")
        return False

    print(f"\nğŸ“¦ Auto-preparing data for {comp_id}...")

    try:
        # è·¯å¾„é…ç½®
        raw_dir = SCIENCEAGENT_DIR / 'datasets' / dataset_name
        data_dir = SCIENCEBENCH_DATA_DIR / comp_id / 'prepared'
        public_dir = data_dir / 'public'
        private_dir = data_dir / 'private'

        print(f"  Raw: {raw_dir}")
        print(f"  Public: {public_dir}")
        print(f"  Private: {private_dir}")

        # åˆ›å»ºæ•°æ®ç›®å½•
        public_dir.mkdir(parents=True, exist_ok=True)
        private_dir.mkdir(parents=True, exist_ok=True)

        # è¿è¡Œ prepare å‡½æ•°
        import sys
        sys.path.insert(0, str(comp_dir))

        try:
            from prepare import prepare
            prepare(raw_dir, public_dir, private_dir)
            print(f"âœ… Data prepared successfully!")
            return True
        except Exception as e:
            print(f"âŒ Prepare failed: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            # æ¸…ç†å¯¼å…¥
            if str(comp_dir) in sys.path:
                sys.path.remove(str(comp_dir))
            # ç§»é™¤å·²å¯¼å…¥çš„ prepare æ¨¡å—
            if 'prepare' in sys.modules:
                del sys.modules['prepare']

    except Exception as e:
        print(f"âŒ Error running prepare: {e}")
        import traceback
        traceback.print_exc()
        return False


def list_tasks(df: pd.DataFrame, category: Optional[str] = None):
    """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡"""
    if category:
        df = df[df['domain'].str.contains(category, case=False, na=False)]

    print(f"\n{'='*80}")
    print(f"ScienceAgent-bench Tasks ({len(df)} total)")
    print(f"{'='*80}\n")

    # æŒ‰é¢†åŸŸåˆ†ç»„
    for domain in df['domain'].unique():
        domain_tasks = df[df['domain'] == domain]
        print(f"\n## {domain} ({len(domain_tasks)} tasks)")
        print("-" * 80)

        for _, task in domain_tasks.head(10).iterrows():  # æ¯ä¸ªé¢†åŸŸæœ€å¤šæ˜¾ç¤º10ä¸ª
            comp_id = create_competition_id(task['instance_id'], task['gold_program_name'])
            print(f"  [{task['instance_id']:3d}] {comp_id}")
            print(f"       {task.get('subtask_categories', 'N/A')}")

        if len(domain_tasks) > 10:
            print(f"       ... and {len(domain_tasks) - 10} more tasks")

    print(f"\n{'='*80}\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Convert ScienceAgent-bench tasks to MLE-Bench format'
    )
    parser.add_argument(
        '--instance-ids',
        type=int,
        nargs='+',
        help='Specific instance IDs to convert'
    )
    parser.add_argument(
        '--all',
        action='store_true',
        help='Convert all tasks'
    )
    parser.add_argument(
        '--list',
        action='store_true',
        help='List all available tasks'
    )
    parser.add_argument(
        '--category',
        type=str,
        help='Filter by domain category'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Preview conversion without creating files'
    )
    parser.add_argument(
        '--auto-prepare',
        action='store_true',
        help='Automatically run prepare.py after conversion'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='Limit number of tasks to convert (for testing)'
    )

    args = parser.parse_args()

    # åŠ è½½å…ƒæ•°æ®
    df = load_scienceagent_metadata()

    # åˆ—å‡ºä»»åŠ¡
    if args.list:
        list_tasks(df, args.category)
        return

    # ç¡®å®šè¦è½¬æ¢çš„ä»»åŠ¡
    if args.instance_ids:
        tasks_to_convert = df[df['instance_id'].isin(args.instance_ids)]
    elif args.all:
        tasks_to_convert = df
        if args.category:
            tasks_to_convert = tasks_to_convert[
                tasks_to_convert['domain'].str.contains(args.category, case=False, na=False)
            ]
    else:
        print("âŒ Error: Please specify --instance-ids, --all, or --list")
        parser.print_help()
        return

    # é™åˆ¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    if args.limit:
        tasks_to_convert = tasks_to_convert.head(args.limit)

    print(f"\nğŸš€ Converting {len(tasks_to_convert)} task(s)...")
    if args.auto_prepare:
        print("âš¡ Auto-prepare mode enabled\n")
    if args.dry_run:
        print("ğŸ” DRY RUN mode - no files will be created\n")

    # åˆ›å»ºç›®æ ‡ç›®å½•
    if not args.dry_run:
        SCIENCEBENCH_REGISTRY_DIR.mkdir(parents=True, exist_ok=True)
        SCIENCEBENCH_DATA_DIR.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ Registry directory: {SCIENCEBENCH_REGISTRY_DIR}")
        print(f"ğŸ“ Data directory: {SCIENCEBENCH_DATA_DIR}\n")

    # è½¬æ¢ä»»åŠ¡
    success_count = 0
    failed_tasks = []

    for _, task_data in tasks_to_convert.iterrows():
        task_dict = task_data.to_dict()
        instance_id = task_dict['instance_id']

        success = convert_task(instance_id, task_dict, args.dry_run)

        if success:
            success_count += 1

            # Auto-prepare
            if args.auto_prepare and not args.dry_run:
                comp_id = create_competition_id(instance_id, task_dict.get('gold_program_name'))
                dataset_name = extract_dataset_name(task_dict.get('dataset_folder_tree'))
                run_auto_prepare(comp_id, dataset_name)
        else:
            failed_tasks.append(instance_id)

    # æ€»ç»“
    print(f"\n{'='*60}")
    print(f"Conversion Summary")
    print(f"{'='*60}")
    print(f"âœ… Success: {success_count}/{len(tasks_to_convert)}")
    if failed_tasks:
        print(f"âŒ Failed: {len(failed_tasks)} tasks")
        print(f"   Task IDs: {failed_tasks}")
    print(f"{'='*60}\n")


if __name__ == '__main__':
    main()
