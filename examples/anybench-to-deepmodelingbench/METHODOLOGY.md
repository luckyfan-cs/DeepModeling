# ä»»æ„ Benchmark è½¬æ¢åˆ° DeepModeling/MLE-Bench æ ¼å¼çš„æ–¹æ³•è®º

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº†ä¸€å¥—é€šç”¨çš„æ–¹æ³•è®ºï¼Œç”¨äºå°†ä»»æ„æ•°æ®é›†/åŸºå‡†æµ‹è¯•è½¬æ¢ä¸º DeepModeling/MLE-Bench æ ¼å¼ã€‚æ­¤æ–¹æ³•è®ºåŸºäº DABench åˆ° MLE-Bench çš„æˆåŠŸè½¬æ¢ç»éªŒã€‚

## ğŸ¯ æ ¸å¿ƒç†å¿µ

**å…³é”®ç›®æ ‡**: å°†ä»»ä½•æ•°æ®ç§‘å­¦ä»»åŠ¡è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„æ¯”èµ›æ ¼å¼ï¼Œä½¿å…¶å¯ä»¥è¢«è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿè¿è¡Œå’Œè¯„åˆ†ã€‚

### ä¸‰ä¸ªæ ¸å¿ƒç›®å½•
- **Benchmark æ³¨å†Œç›®å½•**: å­˜æ”¾æ¯”èµ›å®šä¹‰å’Œè¯„ä¼°é€»è¾‘
- **æ•°æ®æºç›®å½•**: å­˜æ”¾åŸå§‹æ•°æ®ï¼ˆä¿æŒä¸å˜ï¼‰
- **è½¬æ¢è„šæœ¬ç›®å½•**: å­˜æ”¾æ‰¹é‡è½¬æ¢å·¥å…·

## ğŸ“‚ æ ‡å‡†ç›®å½•ç»“æ„

```
/home/aiops/liufan/projects/
â”œâ”€â”€ DeepModeling/
â”‚   â”œâ”€â”€ benchmarks/
â”‚   â”‚   â””â”€â”€ <benchmark-name>/           # æ¯”èµ›æ³¨å†Œç›®å½•
â”‚   â”‚       â””â”€â”€ competitions/
â”‚   â”‚           â””â”€â”€ <task-id>/
â”‚   â”‚               â”œâ”€â”€ config.yaml      # æ¯”èµ›é…ç½® â­
â”‚   â”‚               â”œâ”€â”€ description.md   # ä»»åŠ¡æè¿°
â”‚   â”‚               â”œâ”€â”€ grade.py         # è¯„åˆ†å‡½æ•° â­
â”‚   â”‚               â”œâ”€â”€ prepare.py       # æ•°æ®å‡†å¤‡å‡½æ•° â­
â”‚   â”‚               â”œâ”€â”€ leaderboard.csv  # æ’è¡Œæ¦œï¼ˆå¯é€‰ï¼‰
â”‚   â”‚               â””â”€â”€ checksums.yaml   # æ•°æ®æ ¡éªŒï¼ˆå¯é€‰ï¼‰
â”‚   â””â”€â”€ examples/
â”‚       â”œâ”€â”€ anybench-to-deepmodelingbench/  # æ–¹æ³•è®ºæ–‡æ¡£ï¼ˆæœ¬æ–‡æ¡£ï¼‰
â”‚       â””â”€â”€ <source>-to-mlebench/           # å…·ä½“è½¬æ¢è„šæœ¬
â”‚           â”œâ”€â”€ README.md
â”‚           â”œâ”€â”€ convert_<source>_to_mlebench.py
â”‚           â””â”€â”€ ... (å…¶ä»–è¾…åŠ©è„šæœ¬)
â””â”€â”€ <SourceBenchmark>/                  # åŸå§‹æ•°æ®æºç›®å½•
    â””â”€â”€ benchmark/
        â””â”€â”€ <task-id>/
            â”œâ”€â”€ public/                  # å…¬å¼€æ•°æ®
            â”œâ”€â”€ private/                 # ç§æœ‰æ•°æ®ï¼ˆç­”æ¡ˆï¼‰
            â””â”€â”€ ... (å…¶ä»–åŸå§‹æ–‡ä»¶)
```

## ğŸ”‘ å…­ä¸ªæ ¸å¿ƒæ–‡ä»¶

### 1. config.yaml - æ¯”èµ›é…ç½®æ–‡ä»¶

**ä½œç”¨**: å®šä¹‰æ¯”èµ›çš„å…ƒæ•°æ®å’Œè·¯å¾„

**æ¨¡æ¿**:
```yaml
id: <competition-id>
name: "<Competition Name>"
competition_type: code  # æˆ– notebook
awards_medals: false
prizes: null
description: mlebench/competitions/<competition-id>/description.md

dataset:
  answers: <competition-id>/prepared/private/answer.csv
  sample_submission: <competition-id>/prepared/public/sample_submission.csv

grader:
  name: <metric-name>  # accuracy, rmse, exact_match ç­‰
  grade_fn: mlebench.competitions.<competition-id>.grade:grade

preparer: mlebench.competitions.<competition-id>.prepare:prepare
```

**å…³é”®å†³ç­–ç‚¹**:
- **competition_type**: code è¿˜æ˜¯ notebook
- **grader.name**: é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡
- **dataset paths**: ç¡®ä¿è·¯å¾„ä¸æ•°æ®å‡†å¤‡å‡½æ•°ä¸€è‡´

### 2. description.md - ä»»åŠ¡æè¿°

**ä½œç”¨**: æä¾›ä»»åŠ¡çš„è¯¦ç»†æè¿°ï¼Œä¾› Agent ç†è§£ä»»åŠ¡

**ç»“æ„**:
```markdown
# <Task Name>

## Task Description
<è¯¦ç»†æè¿°ä»»åŠ¡ç›®æ ‡>

## Dataset Description
- **Training Data**: <æè¿°è®­ç»ƒæ•°æ®>
- **Test Data**: <æè¿°æµ‹è¯•æ•°æ®>
- **Target**: <æè¿°é¢„æµ‹ç›®æ ‡>

## Evaluation Metric
<æè¿°è¯„ä¼°æ–¹å¼>

## Data Fields
- `field1`: <æè¿°>
- `field2`: <æè¿°>

## Submission Format
<æè¿°æäº¤æ–‡ä»¶æ ¼å¼>
```

### 3. prepare.py - æ•°æ®å‡†å¤‡å‡½æ•°

**ä½œç”¨**: å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼

**æ ¸å¿ƒå‡½æ•°ç­¾å**:
```python
from pathlib import Path

def prepare(raw: Path, public: Path, private: Path):
    """
    å‡†å¤‡æ•°æ®é›†

    Args:
        raw: åŸå§‹æ•°æ®ç›®å½•
        public: å…¬å¼€æ•°æ®ç›®å½•ï¼ˆå‚èµ›è€…å¯è§ï¼‰
        private: ç§æœ‰æ•°æ®ç›®å½•ï¼ˆä»…ç”¨äºè¯„åˆ†ï¼‰
    """
    # 1. ä» raw è¯»å–åŸå§‹æ•°æ®
    # 2. å¤„ç†å¹¶åˆ†å‰²æ•°æ®
    # 3. ä¿å­˜åˆ° public å’Œ private
    pass
```

**æ ‡å‡†è¾“å‡º**:
- **public/**:
  - `train.csv` æˆ– `train_data.npy`: è®­ç»ƒæ•°æ®
  - `sample_submission.csv`: æäº¤æ ·æœ¬
- **private/**:
  - `answer.csv` æˆ– `test_labels.csv`: æµ‹è¯•é›†ç­”æ¡ˆ

**æœ€ä½³å®è·µ**:
1. âœ… æ·»åŠ è¯¦ç»†çš„æ•°æ®åŠ è½½æ—¥å¿—
2. âœ… åŒ…å«æ•°æ®éªŒè¯æ£€æŸ¥
3. âœ… å¤„ç†å¼‚å¸¸æƒ…å†µï¼ˆtry-exceptï¼‰
4. âœ… ä½¿ç”¨ assert éªŒè¯è¾“å‡ºæ–‡ä»¶å­˜åœ¨

### 4. grade.py - è¯„åˆ†å‡½æ•°

**ä½œç”¨**: æ¯”è¾ƒæäº¤ç»“æœå’Œæ­£ç¡®ç­”æ¡ˆï¼Œè®¡ç®—å¾—åˆ†

**æ ¸å¿ƒå‡½æ•°ç­¾å**:
```python
import pandas as pd

def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """
    è¯„åˆ†å‡½æ•°

    Args:
        submission: å‚èµ›è€…æäº¤çš„é¢„æµ‹ç»“æœ
        answers: æ­£ç¡®ç­”æ¡ˆ

    Returns:
        float: å¾—åˆ†ï¼ˆé€šå¸¸åœ¨ 0-1 ä¹‹é—´ï¼‰
    """
    # 1. æ•°æ®å¯¹é½ï¼ˆmerge on idï¼‰
    # 2. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    # 3. è¿”å›å¾—åˆ†
    pass
```

**å¸¸è§è¯„ä¼°æŒ‡æ ‡**:
- **å‡†ç¡®ç‡**: `sklearn.metrics.accuracy_score`
- **RMSE**: `sklearn.metrics.mean_squared_error(squared=False)`
- **ç²¾ç¡®åŒ¹é…**: è‡ªå®šä¹‰å­—ç¬¦ä¸²åŒ¹é…é€»è¾‘
- **F1 åˆ†æ•°**: `sklearn.metrics.f1_score`

### 5. leaderboard.csv - æ’è¡Œæ¦œï¼ˆå¯é€‰ï¼‰

**ä½œç”¨**: è®°å½•åŸºå‡†åˆ†æ•°

**æ ¼å¼**:
```csv
submission_id,score,username,date
baseline,0.75,random_baseline,2024-01-01
human,0.90,human_expert,2024-01-01
```

### 6. checksums.yaml - æ•°æ®æ ¡éªŒï¼ˆå¯é€‰ï¼‰

**ä½œç”¨**: éªŒè¯æ•°æ®å®Œæ•´æ€§

**æ ¼å¼**:
```yaml
public:
  train.csv: <md5-hash>
  sample_submission.csv: <md5-hash>
private:
  answer.csv: <md5-hash>
```

## ğŸ”„ è½¬æ¢æµç¨‹ (7æ­¥æ³•)

### Step 1: åˆ†ææºæ•°æ®é›†ç»“æ„

**ä»»åŠ¡æ¸…å•**:
- [ ] ç¡®å®šæœ‰å¤šå°‘ä¸ªä»»åŠ¡/é¢˜ç›®
- [ ] æ‰¾åˆ°æ•°æ®æ–‡ä»¶ä½ç½®
- [ ] äº†è§£æ•°æ®æ ¼å¼ï¼ˆCSV, JSON, NPY ç­‰ï¼‰
- [ ] ç¡®å®šè¯„ä¼°æ–¹å¼å’Œç­”æ¡ˆæ ¼å¼

**è¾“å‡º**: æºæ•°æ®é›†çš„ç»“æ„æ–‡æ¡£

### Step 2: è®¾è®¡ competition_id å‘½åè§„åˆ™

**å‘½åè§„èŒƒ**:
```
<source>-<task-id>-<keywords>
```

**ç¤ºä¾‹**:
- DABench: `dabench-0-mean-fare`
- ScienceAgent: `sciencebench-01-glacier-plot`
- MathModeling: `mathmodel-a-population-growth`

**æœ€ä½³å®è·µ**:
- ä½¿ç”¨å°å†™å­—æ¯å’Œè¿å­—ç¬¦
- åŒ…å«æ•°å­— ID ä»¥ä¿è¯å”¯ä¸€æ€§
- æ·»åŠ  2-3 ä¸ªæè¿°æ€§å…³é”®è¯

### Step 3: åˆ›å»ºæ•°æ®æ˜ å°„é€»è¾‘

**å…³é”®é—®é¢˜**:
1. **è®­ç»ƒ/æµ‹è¯•åˆ†å‰²**: å¦‚ä½•åˆ’åˆ†ï¼Ÿæ˜¯å¦å·²ç»åˆ’åˆ†ï¼Ÿ
2. **ç­”æ¡ˆæå–**: ç­”æ¡ˆå­˜å‚¨åœ¨å“ªé‡Œï¼Ÿæ ¼å¼æ˜¯ä»€ä¹ˆï¼Ÿ
3. **æ–‡ä»¶æ ¼å¼**: CSV, JSON, NPY, è¿˜æ˜¯å…¶ä»–ï¼Ÿ

**æ•°æ®æµå›¾**:
```
æºæ•°æ® â†’ prepare.py â†’ public/  (train.csv, sample_submission.csv)
                   â†’ private/ (answer.csv)
```

### Step 4: ç¼–å†™è½¬æ¢è„šæœ¬

**æ ¸å¿ƒè„šæœ¬ç»“æ„**:
```python
# convert_<source>_to_mlebench.py

import argparse
from pathlib import Path

# 1. é…ç½®è·¯å¾„
SOURCE_DIR = Path('/path/to/source')
COMPETITIONS_DIR = Path('/path/to/DeepModeling/benchmarks/<name>/competitions')

# 2. åŠ è½½æºæ•°æ®
def load_source_data():
    pass

# 3. ç”Ÿæˆå…­ä¸ªæ ¸å¿ƒæ–‡ä»¶
def create_config_yaml(task_id):
    pass

def create_description_md(task_data):
    pass

def create_prepare_py(task_data):
    pass

def create_grade_py(task_data):
    pass

# 4. æ‰¹é‡è½¬æ¢å‡½æ•°
def convert_task(task_id):
    # 4.1 åˆ›å»ºç›®å½•
    # 4.2 ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶
    # 4.3 éªŒè¯
    pass

# 5. ä¸»å‡½æ•°
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--task-ids', nargs='+', type=int)
    parser.add_argument('--all', action='store_true')
    parser.add_argument('--auto-prepare', action='store_true')
    parser.add_argument('--dry-run', action='store_true')
    args = parser.parse_args()

    # æ‰¹é‡è½¬æ¢
    for task_id in task_ids:
        convert_task(task_id)

        if args.auto_prepare:
            # è‡ªåŠ¨è¿è¡Œ prepare.py
            run_prepare(task_id)

if __name__ == '__main__':
    main()
```

### Step 5: å®ç° auto-prepare åŠŸèƒ½

**ä½œç”¨**: è½¬æ¢åè‡ªåŠ¨å‡†å¤‡æ•°æ®ï¼Œé¿å…æ‰‹åŠ¨æ“ä½œ

**å®ç°**:
```python
import subprocess

def run_prepare(competition_id: str, data_dir: Path):
    """è‡ªåŠ¨è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬"""
    prepare_script = data_dir / competition_id / "prepare.py"

    if not prepare_script.exists():
        print(f"âš  Prepare script not found: {prepare_script}")
        return False

    print(f"ğŸ“¦ Auto-preparing data for {competition_id}...")

    try:
        result = subprocess.run(
            ["python", str(prepare_script)],
            cwd=str(data_dir / competition_id),
            timeout=60,
            capture_output=True,
            text=True
        )

        if result.returncode == 0:
            print(f"âœ… Data prepared successfully!")
            return True
        else:
            print(f"âŒ Prepare failed: {result.stderr}")
            return False

    except subprocess.TimeoutExpired:
        print(f"âŒ Prepare timeout (60s)")
        return False
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False
```

### Step 6: éªŒè¯è½¬æ¢ç»“æœ

**éªŒè¯æ¸…å•**:
- [ ] æ‰€æœ‰å¿…éœ€æ–‡ä»¶éƒ½å·²åˆ›å»º
- [ ] config.yaml æ ¼å¼æ­£ç¡®
- [ ] prepare.py å¯ä»¥æˆåŠŸè¿è¡Œ
- [ ] public/ å’Œ private/ ç›®å½•åŒ…å«æ­£ç¡®çš„æ–‡ä»¶
- [ ] grade.py å¯ä»¥æ­£ç¡®è¯„åˆ†

**è‡ªåŠ¨åŒ–éªŒè¯è„šæœ¬**:
```python
def verify_competition(competition_dir: Path):
    """éªŒè¯æ¯”èµ›æ–‡ä»¶å®Œæ•´æ€§"""
    required_files = [
        'config.yaml',
        'description.md',
        'grade.py',
        'prepare.py'
    ]

    for file in required_files:
        assert (competition_dir / file).exists(), f"Missing {file}"

    print("âœ… All files present")
```

### Step 7: è¿è¡Œæµ‹è¯•

**æµ‹è¯•æµç¨‹**:
```bash
# 1. è½¬æ¢å•ä¸ªä»»åŠ¡
python convert_<source>_to_mlebench.py --task-ids 0 --auto-prepare

# 2. éªŒè¯æ•°æ®æ–‡ä»¶
ls <data-dir>/<competition-id>/prepared/public/
ls <data-dir>/<competition-id>/prepared/private/

# 3. è¿è¡Œæ¯”èµ›æµ‹è¯•
cd /home/aiops/liufan/projects/DeepModeling
python main.py \
  --benchmark <benchmark-name> \
  --competitions <competition-id> \
  --max-steps 5

# 4. æ£€æŸ¥ç»“æœ
cat runs/benchmark_results/*/results.json
```

## ğŸ¨ å¸¸è§æ•°æ®æ ¼å¼è½¬æ¢æ¨¡å¼

### æ¨¡å¼ 1: è¡¨æ ¼æ•°æ® (CSV)

**æºæ ¼å¼**:
```
data/
  train.csv
  test.csv
  answers.csv
```

**prepare.py æ¨¡æ¿**:
```python
def prepare(raw: Path, public: Path, private: Path):
    # è¯»å–æ•°æ®
    train = pd.read_csv(raw / "train.csv")
    test = pd.read_csv(raw / "test.csv")
    answers = pd.read_csv(raw / "answers.csv")

    # ä¿å­˜åˆ° public
    train.to_csv(public / "train.csv", index=False)
    test.to_csv(public / "test.csv", index=False)

    # åˆ›å»ºæäº¤æ ·æœ¬
    sample = pd.DataFrame({"id": test["id"], "target": 0})
    sample.to_csv(public / "sample_submission.csv", index=False)

    # ä¿å­˜ç­”æ¡ˆåˆ° private
    answers.to_csv(private / "answer.csv", index=False)
```

### æ¨¡å¼ 2: æ•°ç»„æ•°æ® (NumPy)

**æºæ ¼å¼**:
```
data/
  train_data.npy
  train_labels.npy
  test_data.npy
  test_labels.npy
```

**prepare.py æ¨¡æ¿**:
```python
import numpy as np

def prepare(raw: Path, public: Path, private: Path):
    # åŠ è½½æ•°æ®
    X_train = np.load(raw / "train_data.npy")
    y_train = np.load(raw / "train_labels.npy")
    X_test = np.load(raw / "test_data.npy")
    y_test = np.load(raw / "test_labels.npy")

    # ä¿å­˜åˆ° public
    np.save(public / "train_data.npy", X_train)
    np.save(public / "train_labels.npy", y_train)
    np.save(public / "test_data.npy", X_test)

    # åˆ›å»ºæäº¤æ ·æœ¬
    sample_df = pd.DataFrame({
        "id": range(len(y_test)),
        "label": 0
    })
    sample_df.to_csv(public / "sample_submission.csv", index=False)

    # ä¿å­˜ç­”æ¡ˆ
    answer_df = pd.DataFrame({
        "id": range(len(y_test)),
        "label": y_test
    })
    answer_df.to_csv(private / "test_labels.csv", index=False)
```

### æ¨¡å¼ 3: JSON æ•°æ®

**æºæ ¼å¼**:
```
tasks.json
answers.json
datasets/
```

**prepare.py æ¨¡æ¿**:
```python
import json

def prepare(raw: Path, public: Path, private: Path):
    # åŠ è½½ä»»åŠ¡å®šä¹‰
    with open(raw / "tasks.json") as f:
        tasks = json.load(f)

    with open(raw / "answers.json") as f:
        answers = json.load(f)

    # åŠ è½½å’Œå¤„ç†æ•°æ®é›†
    # ... (å…·ä½“é€»è¾‘å–å†³äºæ•°æ®æ ¼å¼)

    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    # ...
```

### æ¨¡å¼ 4: æ–‡æœ¬ç­”æ¡ˆæ ¼å¼

**é€‚ç”¨åœºæ™¯**: DABench é£æ ¼çš„ç­”æ¡ˆæ ¼å¼ `@key[value]`

**grade.py æ¨¡æ¿**:
```python
import re

def parse_answer(answer_str: str) -> dict:
    """è§£æ @key[value] æ ¼å¼çš„ç­”æ¡ˆ"""
    pattern = r'@(\w+)\[([^\]]+)\]'
    matches = re.findall(pattern, answer_str)
    return {key: value for key, value in matches}

def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """ç²¾ç¡®åŒ¹é…è¯„åˆ†"""
    total = len(answers)
    correct = 0

    for idx, row in answers.iterrows():
        true_answer = parse_answer(row['answer'])
        pred_answer = parse_answer(submission.loc[idx, 'answer'])

        if true_answer == pred_answer:
            correct += 1

    return correct / total
```

## ğŸš€ é«˜çº§ç‰¹æ€§

### ç‰¹æ€§ 1: å¤šè¯„ä¼°æŒ‡æ ‡æ”¯æŒ

æœ‰äº›ä»»åŠ¡å¯èƒ½éœ€è¦å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚

**å®ç°**:
```python
# grade.py
def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """è¿”å›ä¸»è¦æŒ‡æ ‡"""
    accuracy = calculate_accuracy(submission, answers)
    return accuracy

def grade_detailed(submission: pd.DataFrame, answers: pd.DataFrame) -> dict:
    """è¿”å›è¯¦ç»†æŒ‡æ ‡"""
    return {
        "accuracy": calculate_accuracy(submission, answers),
        "f1": calculate_f1(submission, answers),
        "precision": calculate_precision(submission, answers),
        "recall": calculate_recall(submission, answers)
    }
```

### ç‰¹æ€§ 2: æ•°æ®é›†ç‰ˆæœ¬æ§åˆ¶

ä½¿ç”¨ checksums.yaml ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚

**ç”Ÿæˆ checksums**:
```python
import hashlib

def calculate_md5(file_path: Path) -> str:
    """è®¡ç®—æ–‡ä»¶ MD5"""
    md5 = hashlib.md5()
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            md5.update(chunk)
    return md5.hexdigest()

def generate_checksums(data_dir: Path) -> dict:
    """ç”Ÿæˆæ‰€æœ‰æ•°æ®æ–‡ä»¶çš„æ ¡éªŒå’Œ"""
    checksums = {"public": {}, "private": {}}

    for file in (data_dir / "public").glob("*"):
        checksums["public"][file.name] = calculate_md5(file)

    for file in (data_dir / "private").glob("*"):
        checksums["private"][file.name] = calculate_md5(file)

    return checksums
```

### ç‰¹æ€§ 3: å¹¶è¡Œè½¬æ¢

å¯¹äºå¤§é‡ä»»åŠ¡ï¼Œä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿè½¬æ¢ã€‚

**å®ç°**:
```python
from multiprocessing import Pool

def convert_task_wrapper(args):
    """åŒ…è£…å‡½æ•°ç”¨äºå¤šè¿›ç¨‹"""
    task_id, config = args
    try:
        convert_task(task_id, config)
        return task_id, True
    except Exception as e:
        return task_id, False

def batch_convert(task_ids: list, num_workers: int = 4):
    """å¹¶è¡Œè½¬æ¢å¤šä¸ªä»»åŠ¡"""
    with Pool(num_workers) as pool:
        results = pool.map(convert_task_wrapper, task_ids)

    success = [tid for tid, ok in results if ok]
    failed = [tid for tid, ok in results if not ok]

    print(f"âœ… Success: {len(success)}")
    print(f"âŒ Failed: {len(failed)}")
```

## ğŸ“Š è´¨é‡æ£€æŸ¥æ¸…å•

### è½¬æ¢å‰æ£€æŸ¥
- [ ] æºæ•°æ®é›†å®Œæ•´ä¸‹è½½
- [ ] äº†è§£æ•°æ®æ ¼å¼å’Œç»“æ„
- [ ] æ˜ç¡®è¯„ä¼°æŒ‡æ ‡
- [ ] ç¡®å®šç­”æ¡ˆä½ç½®

### è½¬æ¢åæ£€æŸ¥
- [ ] æ‰€æœ‰å¿…éœ€æ–‡ä»¶å­˜åœ¨
- [ ] config.yaml æ ¼å¼æ­£ç¡®
- [ ] description.md æè¿°æ¸…æ™°
- [ ] prepare.py å¯ä»¥è¿è¡Œ
- [ ] grade.py é€»è¾‘æ­£ç¡®
- [ ] æ•°æ®æ–‡ä»¶ç”Ÿæˆæ­£ç¡®

### è¿è¡Œæ—¶æ£€æŸ¥
- [ ] æ¯”èµ›å¯ä»¥æˆåŠŸåŠ è½½
- [ ] Agent èƒ½ç†è§£ä»»åŠ¡æè¿°
- [ ] æäº¤æ–‡ä»¶æ ¼å¼æ­£ç¡®
- [ ] è¯„åˆ†ç»“æœåˆç†
- [ ] æ— é”™è¯¯æˆ–å¼‚å¸¸

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ¸è¿›å¼å¼€å‘
- å…ˆè½¬æ¢ 1-2 ä¸ªä»»åŠ¡æµ‹è¯•
- éªŒè¯å®Œæ•´æµç¨‹åå†æ‰¹é‡è½¬æ¢
- ä½¿ç”¨ `--dry-run` é¢„è§ˆè½¬æ¢

### 2. è¯¦ç»†çš„æ—¥å¿—è¾“å‡º
```python
print(f"âœ… Success: {message}")
print(f"âŒ Error: {message}")
print(f"âš  Warning: {message}")
print(f"ğŸ“¦ Processing: {message}")
print(f"ğŸ” Found: {message}")
```

### 3. é”™è¯¯å¤„ç†
```python
try:
    # ä¸»è¦é€»è¾‘
    pass
except FileNotFoundError as e:
    print(f"âŒ File not found: {e}")
    # é™çº§å¤„ç†
except Exception as e:
    print(f"âŒ Unexpected error: {e}")
    import traceback
    traceback.print_exc()
```

### 4. æ•°æ®éªŒè¯
```python
# åœ¨ prepare.py ä¸­æ·»åŠ éªŒè¯
assert (public / "train.csv").exists(), "Training data missing"
assert len(train) > 0, "Training data is empty"
assert set(train.columns) == expected_columns, "Column mismatch"
```

### 5. æ–‡æ¡£ä¼˜å…ˆ
- README.md å†™æ¸…æ¥šä½¿ç”¨æ–¹æ³•
- æ·»åŠ ç¤ºä¾‹å‘½ä»¤
- è®°å½•å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

## ğŸ”— å‚è€ƒèµ„æº

- **DABench è½¬æ¢ç¤ºä¾‹**: `/home/aiops/liufan/projects/data_science_agent_toolkit/examples/dabench_to_mlebench/`
- **MLE-Bench æ–‡æ¡£**: `/home/aiops/liufan/projects/DeepModeling/benchmarks/mlebench/`
- **é…ç½®ç¤ºä¾‹**: `/home/aiops/liufan/projects/DeepModeling/benchmarks/mlebench/competitions/ethanol-concentration/`

## ğŸ“ æ€»ç»“

è½¬æ¢ä»»æ„ benchmark åˆ° MLE-Bench æ ¼å¼çš„æ ¸å¿ƒæ˜¯ï¼š
1. **ç†è§£æºæ•°æ®ç»“æ„**
2. **æ ‡å‡†åŒ–åˆ°å…­ä¸ªæ ¸å¿ƒæ–‡ä»¶**
3. **å®ç° prepare å’Œ grade å‡½æ•°**
4. **è‡ªåŠ¨åŒ–æ‰¹é‡è½¬æ¢æµç¨‹**
5. **å……åˆ†æµ‹è¯•å’ŒéªŒè¯**

éµå¾ªæœ¬æ–¹æ³•è®ºï¼Œå¯ä»¥é«˜æ•ˆã€æ ‡å‡†åŒ–åœ°å°†ä»»ä½•æ•°æ®é›†è½¬æ¢ä¸º DeepModeling/MLE-Bench æ ¼å¼ã€‚
